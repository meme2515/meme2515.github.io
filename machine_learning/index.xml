<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Machine_learnings on Soon&#39;s Blog</title>
        <link>https://meme2515.github.io/machine_learning/</link>
        <description>Recent content in Machine_learnings on Soon&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 30 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://meme2515.github.io/machine_learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>엘라스틱넷이란? (Elastic Net)</title>
        <link>https://meme2515.github.io/machine_learning/elasticnet/</link>
        <pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/elasticnet/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/elasticnet_1.png" alt="Featured image of post 엘라스틱넷이란? (Elastic Net)" /&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;모델의 과대적합 (Overfitting) 을 방지하는 대표적인 기법으로 모델 규제 방법이 있다.&lt;/li&gt;
&lt;li&gt;회귀 모델의 경우 보통 &lt;strong&gt;모델 가중치를 제한하는 방법을 통해 규제를 가하는데&lt;/strong&gt;, 이를 &lt;strong&gt;정규화 (regularization)&lt;/strong&gt; 라 칭한다. 모델 가중치에 제한을 두는 방법론은 비단 회귀 모델 뿐 아니라 딥러닝 영역에도 적용된다 - &lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;참고 글&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;본 글에서는 대표적인 회귀 모델의 가중치 적용법인 Ridge, Lasso Regression 과 이 두 방법론을 복합적으로 활용하는 엘락스틱넷을 소개한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;regression-analysis&#34;&gt;Regression Analysis&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/elasticnet_4.jpeg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. &lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/linear-regression-explained-1b36f97b7572&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jason Wong - Linear Regression&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;통계적 예측 분석의 기본이 되는 회귀 분석이란, &lt;strong&gt;독립변수 $x$ 에 대응하는 종속변수 $y$ 와 가장 비슷한 값 $\hat{y}$ 을 출력하는 함수 $f(x)$ 를 찾는 과정&lt;/strong&gt;을 의미한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-regression-선형회귀식&#34;&gt;Linear Regression (선형회귀식)&lt;/h3&gt;
&lt;p&gt;$$
\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + &amp;hellip; + \theta_D x_D
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;만약 $f(x)$ 가 위와 같은 선형함수인 경우 해당 함수를 &lt;strong&gt;선형회귀모형 (Linear Regression Model)&lt;/strong&gt; 이라 칭한다.&lt;/li&gt;
&lt;li&gt;위 식에서 독립변수 $x = (x_1, x_2, &amp;hellip; , x_D)$ 는 D차원의 벡터이며, 가중치 벡터 $w = (w_0, &amp;hellip; , w_D)$ 는 함수 $f(x)$ 의 &lt;strong&gt;계수 (coefficient)&lt;/strong&gt; 이자 회귀 모형의 &lt;strong&gt;모수 (parameter)&lt;/strong&gt; 라고 부르게된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;polynomial-regression-다항회귀식&#34;&gt;Polynomial Regression (다항회귀식)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/elasticnet_6.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. &lt;a class=&#34;link&#34; href=&#34;https://www.javatpoint.com/machine-learning-polynomial-regression&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Java Point - ML Polynomial Regression&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;독립변수와 종속변수가 비선형 연관성을 가질 경우, 독립변수의 차수를 높여 다항식 함수를 활용할 수 있으며 이를 &lt;strong&gt;다항회귀식&lt;/strong&gt;이라 부른다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2 + \theta_4 x_2^2 + \theta_5 x_1x_2
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;한 개 이상의 독립변수가 존재할 경우 변수 간 영향을 파악하는 것이 가능하다 &lt;em&gt;(예. $x_1x_2$)&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;차수가 지나치게 높을 경우 과적합이 발생할 위험이 상승한다. 이러한 경우 Cross Validation 을 통해 적합한 차수 선정이 가능.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-normal-equation-정규방정식&#34;&gt;The Normal Equation (정규방정식)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;그렇다면 최적의 선형회귀식은 어떻게 계산할 수 있을까?&lt;/li&gt;
&lt;li&gt;데이터가 주어졌을때 최적의 추세선을 그리기위한 대표적인 방법 중 하나는 &lt;strong&gt;최소제곱법 (Ordinary Least Square, OLS)&lt;/strong&gt; 이다.&lt;/li&gt;
&lt;li&gt;최소제곱법은 &lt;strong&gt;실제 데이터 $y$ 와 추세선의 예측값 $\hat{y}$ 간 차이 (잔차, Residual) 의 제곱합을 최소화하는 추세선을 찾는 방법&lt;/strong&gt;이다. 아래 그림 참고.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/elasticnet_5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 3. &lt;a class=&#34;link&#34; href=&#34;https://gregorygundersen.com/blog/2020/01/04/ols/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Gregory Gunderson - Ordinary Least Squares&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;잔차의 제곱합을 최소화하는 선형함수의 계수는 다음과 같은 행렬연산을 통해 계산이 가능하며, 해당식을 &lt;strong&gt;정규방정식&lt;/strong&gt;이라 부른다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\theta} = (X^T X)^{-1} X^T y
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;주의할 점은 OLS 는 &lt;strong&gt;선형회귀모델의 추정 방법 중 하나일 뿐, 선형회귀모델 자체는 아니라는 점&lt;/strong&gt;이다. 선형회귀문제 해결을 위해 경사하강법 활용 또한 가능하다.&lt;/li&gt;
&lt;li&gt;하지만 OLS 방법론은 경사하강법 과는 달리 한번의 연산으로 최적 계수 산정이 가능하다.&lt;/li&gt;
&lt;li&gt;무려 1800년대 초 Gaussian Distribution 의 그 &lt;a class=&#34;link&#34; href=&#34;https://namu.wiki/w/%EC%B9%B4%EB%A5%BC%20%ED%94%84%EB%A6%AC%EB%93%9C%EB%A6%AC%ED%9E%88%20%EA%B0%80%EC%9A%B0%EC%8A%A4&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;가우스&lt;/a&gt;가 해당 기법을 활용해 소행성 세레스의 궤도를 예측하며 알려지게 되었다 (소행성의 관측위치 - 데이터 - 와 최소한의 오차를 가지는 궤도 - 선형함수 - 을 찾아라&amp;hellip;!) - &lt;a class=&#34;link&#34; href=&#34;https://ko.wikipedia.org/wiki/%EC%84%B8%EB%A0%88%EC%8A%A4_%28%EC%99%9C%ED%96%89%EC%84%B1%29&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;출처&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ridge-l2-regression&#34;&gt;Ridge (L2) Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ridge Regression 은 통계학에서 능형회귀, 또는 Tikhonov Regularization 이라고 불리며, 다음과 같은 비용함수를 활용해 회귀식을 추정한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
J(\theta) = MSE(\theta) + \alpha \frac{1}{2} \sum_{i=1}^{n} \theta_i^2
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;뒷단의 정규화식 $\alpha \frac{1}{2} \sum_{i=1}^{n} \theta_i^2$ 은 모델 학습과정에서만 감안될 뿐, 실제 예측 과정에서 제외되어야 한다.&lt;/li&gt;
&lt;li&gt;Ridge Regression 이 적용된 선형 함수는 데이터에 대한 잔차뿐 아니라 계수값 또한 최소화하게 되며, 이는 &lt;strong&gt;종속변수 예측에 비핵심적인 계수값을 줄이는 효과&lt;/strong&gt;를 가진다.&lt;/li&gt;
&lt;li&gt;하이퍼파라미터 $\alpha$ 를 통해 규제량을 통제할 수 있으며, 이를 매우 큰 값으로 설정할 경우 모든 계수값에 0 에 근접해지는 효과를 가진다.&lt;/li&gt;
&lt;li&gt;Ridge Regression 수행을 위한 정규방정식은 다음과 같다 ($A$ 는 (n+1) x (n+1) 크기의 Identity Matrix 이며, 가장 왼쪽 상단의 값이 $0$ 이다).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\theta} = (X^T X + \alpha A)^{-1} X^T y
$$&lt;/p&gt;
&lt;h2 id=&#34;lasso-l1-regression&#34;&gt;Lasso (L1) Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lasso (Least Absolute Shrinkage and Selection Operator) Regression 은 기본적으로 Ridge Regression 과 동일하지만 계수의 제곱값이 아닌 절대값을 활용한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
J(\theta) = MSE(\theta) + \alpha \sum_{i=1}^{n} |\theta_i|
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lasso Regression 의 가장 중요한 성질은 &lt;strong&gt;종속변수 예측에 비핵심적인 변수를 모델에서 제외하는 효과&lt;/strong&gt;를 가진다는 점이다. 이는 해당 변수의 계수를 낮게 산정하지만, 제외하지는 않는 Ridge Regression 과 차이를 가진다.&lt;/li&gt;
&lt;li&gt;상기된 효과는 일종의 Feature Selection 과정으로 해석하는 것이 가능하다.&lt;/li&gt;
&lt;li&gt;이러한 효과가 나타나는 이유는 &lt;strong&gt;L1 Norm (절대값) 은 크기에 관계없이 모든 계수를 동일하게 취급하는 반면, L2 Norm (제곱값) 은 크기가 큰 계수에 더욱 큰 가중치를 부여하기 때문&lt;/strong&gt;이다. 따라서 최적화 시 계수의 감소 속도에 차이를 가지게 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/elasticnet_7.pbm&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 4. &lt;a class=&#34;link&#34; href=&#34;https://www.researchgate.net/figure/Parameter-norm-penalties-L2-norm-regularization-left-and-L1-norm-regularization_fig2_355020694&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Research Gate - L1 vs. L2 Norm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;위 그림은 L1 vs. L2 정규화 함수가 동일한 손실값을 산정하는 영역을 나타낸다. 제곱값은 원형으로, 절대값은 사각형으로 묘사되는 것을 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;elastic-net&#34;&gt;Elastic Net&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;엘라스틱넷&lt;/strong&gt;은 Ridge Regression 과 Lasso Regression 을 복합적으로 활용하는 정규화 기법이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
J(\theta) = MSE(\theta) + r\alpha \sum_{i=1}^{n} |\theta_i| + \frac{1-r}{2}\alpha \sum_{i=1}^{n} \theta_i^2
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;하이퍼파라미터 $r$ 을 활용하여 각 기법의 영향도를 조절할 수 있으며, 한가지 기법만을 활용하는 것 또한 가능하다.&lt;/li&gt;
&lt;li&gt;일반적인 회귀 문제에선 정규화를 반드시 포함하는 것이 권장된다. Ridge Regression 은 좋은 시작 지점이나, feature 선별이 필요한 경우 Lasso 또는 엘라스틱넷을 활용하는 편이 좋다.&lt;/li&gt;
&lt;li&gt;Lasso Regression 은 feature 수가 지나치게 많거나 multicollinearity 가 존재하는 경우 이상행동을 보일 수 있으며, 엘라스틱넷의 경우 이러한 문제가 덜한 편.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;O&amp;rsquo;Reilly - Hands-On Machine Learning with Scikit-Learn, Keras &amp;amp; Tensorflow&lt;/li&gt;
&lt;li&gt;StatQuest - Regularization with Regression &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=Q81RR3yKn30&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[1]&lt;/a&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=NGf0voTMlcs&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[2]&lt;/a&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=1dKRdX9bfIo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[3]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://yganalyst.github.io/ml/ML_chap3-4/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Handson ML - 릿지, 라쏘, 엘라스틱 넷&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://datascienceschool.net/03%20machine%20learning/04.02%20%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D%EC%9D%98%20%EA%B8%B0%EC%B4%88.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;데이터 사이언스 스쿨 - 선형회귀분석의 기초&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>인과추론 (Causal Inference) 개요</title>
        <link>https://meme2515.github.io/machine_learning/causal_inference/</link>
        <pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/causal_inference/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/causal_inference_4.webp" alt="Featured image of post 인과추론 (Causal Inference) 개요" /&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;인과추론이란 &lt;strong&gt;최종적인 결과값에 영향을 주는 독립변수를 파악해, 이를 조절하는 것에 목적&lt;/strong&gt;을 두며 예측과제와 상반된 목적을 가진다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/causal_inference_2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. Prediction vs. Causal Inference, &lt;a class=&#34;link&#34; href=&#34;https://yeong-jin-data-blog.tistory.com/entry/%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0Causal-Inference-%EA%B0%9C%EC%9A%94&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;벌꿀오소리의 공부 일지&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;인과추론과 예측 과제는 다른 목적을 가지지만 경우에 따라 &lt;em&gt;(ex. 모델링을 위한 변수 선정 과정에서 인과추론 방법론 활용 등)&lt;/em&gt; 상호 보완적인 관계를 가질 수 있다.
&lt;ul&gt;
&lt;li&gt;인과추론과 예측 과제는 인풋 -&amp;gt; 모델 -&amp;gt; 아웃풋 이라는 간단한 파이프라인 내에서 최종적인 관심사가 다를뿐, 서로 밀접한 관계를 맺는다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;인과추론 모델 또한 한계점을 가지기 때문에 결과를 절대적인 답으로 볼 수는 없다. 다만 &lt;strong&gt;합리적이고 구체적인 증거를 기반으로 주장할 수 있는 근거&lt;/strong&gt;를 제시한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/causal_inference_3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. Prediction vs. Causal Inference Approaches, &lt;a class=&#34;link&#34; href=&#34;https://yeong-jin-data-blog.tistory.com/entry/%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0Causal-Inference-%EA%B0%9C%EC%9A%94&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;벌꿀오소리의 공부 일지&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;전통적으로 인과추론은 통계 방법론적 접근이 일반적이었으나, 최근 ML 을 활용한 다양한 방법론이 제시되고 있다.&lt;/li&gt;
&lt;li&gt;회사에서 보통 접하게 되는 A/B 테스팅이란 통계학 기반의 실험적 방법론이며, 이외에도 다양한 접근법이 존재.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;endogeneity-내생성&#34;&gt;Endogeneity (내생성)&lt;/h2&gt;
&lt;h3 id=&#34;문제-정의&#34;&gt;문제 정의&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;변수 간 명확한 인과성을 파악하는데 가장 큰 방해요소는 &lt;strong&gt;내생성 (endogeneity)&lt;/strong&gt; 이다.&lt;/li&gt;
&lt;li&gt;내생성이란 독립 변수가 모델의 오차와 상관성을 가지는 경우를 의미한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
y_i = \alpha + \Beta x_i + \epsilon_i
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;아래와 같은 회귀식에서 유의미한 $\alpha$ 와 $\Beta$ 값을 얻기 위해선 다음 조건이 충족되어야 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
E[{\epsilon}_i | x_i] = 0
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;즉, $\alpha$ 와 $\Beta$ 로 설명되는 변수 $y_i$ 와 $x_i$ 간의 관계에서 설명되지 않는 다른 요인이 작용할 경우, 명확한 관계를 판별하는 것이 불가능해지는 것.&lt;/li&gt;
&lt;li&gt;오차값에 의해 영향을 받는 변수 $x_i$ 는 &lt;strong&gt;내생변수 (endogenuous variable)&lt;/strong&gt; 로 분류된다. 이와 다르게 모델 내 다른 어떤 값으로 부터도 영향을 받지 않는 변수는 &lt;strong&gt;외생변수 (exogenuous variable)&lt;/strong&gt; 로 분류.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;발생-원인&#34;&gt;발생 원인&lt;/h3&gt;
&lt;p&gt;내생성의 주요 발생 원인은 다음과 같이 크게 세종류로 구분이 가능하다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(1) Omitted Variables&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/causal_inference_5.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 3. &lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/causal-inference-with-linear-regression-endogeneity-9d9492663bac&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium - Casual Inference with Linear Regression&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;모델 내 존재하지 않는 변수 $Z$가 독립변수, 종속변수와 상관성을 가지게 될 경우 이를 &lt;strong&gt;교란변수 (Confounding Variable)&lt;/strong&gt; 라 칭한다.&lt;/li&gt;
&lt;li&gt;이러한 교란변수가 회귀식 내에 존재하지 않을 경우 (omitted), 이에 영향을 받는 독립변수는 오차값과 상관성을 가지기 때문에 내생변수가 되어버림.&lt;/li&gt;
&lt;li&gt;회귀식에 교란변수를 더하면, 내생성이 사라지고 기존 독립변수의 상관계수가 올바른 값으로 수정됨.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;(2) Simultaneity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;내생성의 또 다른 원인은 종속변수가 독립변수에 영향을 미치는 경우이다. 이 경우 X -&amp;gt; Y 와 Y -&amp;gt; X 의 인과성이 모두 합당하기 때문에 실제 상관계수 산출에 어려움을 겪을 수 있다.
&lt;ul&gt;
&lt;li&gt;예) 높은 교육 수준이 평균 소득을 높이는 것은 사실이나, 평균 소득이 높은 가구는 교육에 더욱 많은 지출을 하는 것 또한 사실이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;이러한 관계는 &lt;strong&gt;Simultaneity Bias&lt;/strong&gt; 를 발생시키며, 관계에서 정확한 인과성을 파악하는 것이 불가능.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;(3) Measurement Error&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;회귀식의 기본 전제는 모든 데이터가 정확하게 측정되었다는 것이나, 실제 측정 환경에서 오류가 발생했을 가능성이 있다.&lt;/li&gt;
&lt;li&gt;이렇듯 오류가 발생한 측정값과 실제값 간의 차이를 &lt;strong&gt;측정 오차 (Measurement Error)&lt;/strong&gt; 라고 칭한다.&lt;/li&gt;
&lt;li&gt;종속변수 $Y$ 내 측정 오차가 존재하는 경우, 내생성이 발생하지 않는다. 반면 독립변수 $X$ 내 측정 오차가 존재하는 경우 내생성 발생.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;simpsons-paradox&#34;&gt;Simpson&amp;rsquo;s Paradox&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;인과추론 과정에서 발생 가능한 대표적인 내생성 문제의 예시가 &lt;strong&gt;심슨의 역설 (Simpson&amp;rsquo;s Paradox)&lt;/strong&gt; 이다.&lt;/li&gt;
&lt;li&gt;간단히 설명해, &lt;strong&gt;여러 그룹의 자료를 합했을 때의 결과가 각 그룹을 구분했을 때의 결과와 다른 때&lt;/strong&gt;를 의미한다.&lt;/li&gt;
&lt;li&gt;예시로 백신 A와 B 중 특정 질병을 치료하는데 더 나은 효과를 보이는 백신을 선택해야 한다고 가정해보자.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/causal_inference_7.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 4. &lt;a class=&#34;link&#34; href=&#34;https://medium.com/bondata/simpsons-paradox-and-confounding-190a26f9e039&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium - Simpson&amp;rsquo;s Paradox and Confounding&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;아래와 같이, 그룹을 단순하게 백신 A, B 를 맞았을 경우로 분리할 경우 다음과 같은 테이블을 얻을 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/causal_inference_8.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 5. &lt;a class=&#34;link&#34; href=&#34;https://medium.com/bondata/simpsons-paradox-and-confounding-190a26f9e039&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium - Simpson&amp;rsquo;s Paradox and Confounding&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;당연히 상단의 수치만으로 비교하였을 경우, 사망률이 16% 로 더 낮은 백신 A를 선택하는 것이 타당하다.&lt;/li&gt;
&lt;li&gt;하지만 데이터를 세분화해, 환자가 백신을 맞기 전 상태로 나누어 확인하게되면 다음과 같이 반대의 결과값이 도출된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/causal_inference_9.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 6. &lt;a class=&#34;link&#34; href=&#34;https://medium.com/bondata/simpsons-paradox-and-confounding-190a26f9e039&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium - Simpson&amp;rsquo;s Paradox and Confounding&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;이와 같이 비직관적인 결과가 도출되는 이유는 Treatment A 를 처방받은 환자는 대부분 Mild 한 상태에 있었으며, Treatment B 를 처방받은 환자는 대부분 Severe 한 상태에 있었다는 점에 기인한다. 즉, 가중치에 차이가 존재.&lt;/li&gt;
&lt;li&gt;이러한 상황에서 백신을 선택하기 위한 의사결정을 수행하기 위해서는, 문제의 인과 모형 (Causal Structure) 을 감안해야 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Case A : 환자의 상태가 백신을 결정하는 경우&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/causal_inference_10.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 7. &lt;a class=&#34;link&#34; href=&#34;https://medium.com/bondata/simpsons-paradox-and-confounding-190a26f9e039&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium - Simpson&amp;rsquo;s Paradox and Confounding&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;공급량의 차이로 인해 Treatment A 는 비교적 상태가 양호한 환자에게 투약하고, Treatment B 는 상태가 안좋은 환자에게 투약한 경우이다.&lt;/li&gt;
&lt;li&gt;선별 방식의 차이로 다음과 같은 샘플 불균형이 발생한다.
&lt;ul&gt;
&lt;li&gt;백신 A를 투약 받은 그룹은 불균형적으로 사망할 확률이 낮은 환자가 다수를 구성.&lt;/li&gt;
&lt;li&gt;백신 B를 투약 받은 그룹은 불균형적으로 사망할 확률이 높은 환자가 다수를 구성.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;선별 방식이 구조적으로 불균형하기 때문에, 세분화된 분석결과를 참고하는 것이 타당하다 (백신 B 선정).&lt;/li&gt;
&lt;li&gt;이 경우 내생성의 발생 원인은 &lt;strong&gt;백신 배분 방식이라는 교란변수이다&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Case B : 백신이 환자의 상태를 결정하는 경우&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/causal_inference_11.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 8. &lt;a class=&#34;link&#34; href=&#34;https://medium.com/bondata/simpsons-paradox-and-confounding-190a26f9e039&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium - Simpson&amp;rsquo;s Paradox and Confounding&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;공급량의 차이로 인해 대기시간의 차이가 발생하였으며, 상대적으로 오래 기다린 Treatment B 환자의 상태가 더욱 악화된 경우이다.
&lt;ul&gt;
&lt;li&gt;백신 B를 투약 받은 그룹은 투약을 위해 대기하는 시간동안 사망할 확률이 높아지게 된다.&lt;/li&gt;
&lt;li&gt;백신 A를 투약 받은 그룹은 투약을 위해 대기하는 시간이 없어 사망할 확률이 높아지지 않게 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;이러한 경우 선별 방식의 불균형성이 없고, 대기 시간 또한 Treatment 의 특성으로 볼 수 있기 때문에 합산된 분석결과를 참고하는 것이 타당하다 (백신 A 선정).&lt;/li&gt;
&lt;li&gt;이 경우 &lt;strong&gt;인과관계의 복잡성은 증가했지만, 내생성이 발생했다고 볼 수 없다&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;해결-방법&#34;&gt;해결 방법&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/causal_inference_12.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 9. &lt;a class=&#34;link&#34; href=&#34;https://yeong-jin-data-blog.tistory.com/entry/%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0Causal-Inference-%EA%B0%9C%EC%9A%94&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;벌꿀오소리의 공부 일지&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;인과추론 과정에서 발생 가능한 내생성을 통제하기 위해 일반적으로 다음과 같은 3가지 방법을 사용할 수 있다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Research Design&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Randomized Controlled Trial&lt;/strong&gt; : 샘플 수집 과정을 디자인해 내생성을 통제하는 방식. 가장 효과적이지만 현실적인 한계가 존재한다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quasi-Experiment&lt;/strong&gt; : 실험과 관계 없이 발생한 데이터를 기반으로 실험 환경을 모방.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/bondata/instrumental-variable-2-e4ff9ae9ca09&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Local Average Treatment Effect (LATE)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=dGLXUwGCu4A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Selection Model&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/use-causal-graphs-4e3af630cf64&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Causal Graph&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=Od6oAz1Op2k&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CodeEmporium - Causal Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=MFnOYNU5sbk&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CodeEmporium - Causal Inference w/ ML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://yeong-jin-data-blog.tistory.com/entry/%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0Causal-Inference-%EA%B0%9C%EC%9A%94&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;벌꿀오소리의 공부 일지&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/causal-inference-with-linear-regression-endogeneity-9d9492663bac&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium - Casual Inference with Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=lLI-0pK9MD8&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ben Lambert - Endogeneity and Instrumental Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/bondata/simpsons-paradox-and-confounding-190a26f9e039&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Medium - Simpson&amp;rsquo;s Paradox and Confounding&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>모델링을 위한 특성 선별 방법 (Feature Selection)</title>
        <link>https://meme2515.github.io/machine_learning/feature_selection/</link>
        <pubDate>Sun, 09 Jul 2023 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/feature_selection/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/feature_1.webp" alt="Featured image of post 모델링을 위한 특성 선별 방법 (Feature Selection)" /&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;운영되는 서비스에서 파생되는 데이터의 종류는 매우 다양하고, 이 중 모델링에 유용한 데이터를 추려내 활용하는 변수 선별 과정은 어렵지만 필수적인 일이다. 정돈된 방식으로, 유의미한 데이터를 선별해 모델링을 진행하지 않을 경우 발생할 수 있는 문제점은 다음과 같이 정리할 수 있다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;지나친 노이즈로 인해 오버피팅이 발생할 수 있다.&lt;/li&gt;
&lt;li&gt;모델 성능이 저하될 수 있다.&lt;/li&gt;
&lt;li&gt;불필요한 학습 시간이 발생할 수 있다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;실무적인 feature selection 과정에선 문제 환경에 기반한 적절한 가설 설정과 테크니컬한 검증 과정이 병행되어야 한다. 아무리 feature 의 유용성이 수치화된다고 하더라도, 조직이 보유하고 있는 모든 데이터를 활용하는 것은 불가능하기 때문이다.&lt;/p&gt;
&lt;p&gt;다음 글은 비즈니스 적인 가설 설정보다는, 이를 검증하기 위한 테크니컬 방법론을 크게 Filtering, Wrapping, Embedding 세 분류로 정리한다.&lt;/p&gt;
&lt;h2 id=&#34;filter-기반-방법&#34;&gt;Filter 기반 방법&lt;/h2&gt;
&lt;p&gt;변수 간 관계성에 기반해 모델 활용에 유용한 feature 를 선정하는 방식이다. 선택 과정에서 실제 모델링을 진행하지는 않으며, 연산처리가 빠른 대신 실제 모델 적용 시 예기치 못한 결과가 발생할 수 있다는 단점을 가진다. 특성상 통계적 방법론이 주를 이룬다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/feature_4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;mutual-information&#34;&gt;Mutual Information&lt;/h3&gt;
&lt;p&gt;정보이론에서 두 개 변수에 대한 &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Mutual_information&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Mutual information&lt;/a&gt; 이란, 하나의 변수를 통해 다른 변수에 대해 얻을 수 있는 정보의 양을 설명하며, 보편적인 Correlation Coefficient 와 달리 변수 간 선형관계나, 연속성을 요하지 않는다.&lt;/p&gt;
&lt;p&gt;$$
\text{Mutual Information} = \sum_{x\in X}\sum_{y\in Y} p(x,y) \text{log}[\frac{p(x,y)}{p(x)p(y)}]
$$&lt;/p&gt;
&lt;p&gt;위 방정식에서 $p(x,y)$ 는 $x, y$ 변수의 결합확률을, $p(x)$ 와 $p(y)$ 는 각각 $x, y$ 변수의 주변확률을 의미한다. 두 변수 중 하나의 값이 변동하지 않는 경우 Mutual Information 은 $0$ 에 근접한 값을 가지며, 변수 간 정보성이 커질수록 (예. 두 변수가 항상 같은 값을 가지는 경우) Mutual Information 은 큰 값을 가지게 된다.&lt;/p&gt;
&lt;p&gt;변수값이 연속성을 가지는 경우, binning 을 통한 카테고리화가 필요하다. 즉, Mutual Information 은 카테고리 변수 활용을 위한 Feature Selection 방법론으로 생각할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;  &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mutual_info_classif&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;importances&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mutual_info_classif&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;feat_importances&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Series&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;importances&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataframe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataframe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;feat_importances&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;barh&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;teal&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/feature_2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;chi-square-test&#34;&gt;Chi-square Test&lt;/h3&gt;
&lt;p&gt;카이제곱검정이란 특정 변수에 대해 가설적으로 설정한 분포와 실제 관측된 분포 간 차이에 대해 통계적 유의성을 구하는 과정을 뜻한다 (예. 동전을 100번 던졌을때 해당 동전이 fair coin 인지 검증).&lt;/p&gt;
&lt;p&gt;데이터가 주어졌을때 분석가는 인풋 변수와 타겟 변수가 독립적이라는 가설 하에 다음 방정식을 활용해 인풋 변수의 모든 값에 대한 expected frequency 를 구할 수 있다.&lt;/p&gt;
&lt;p&gt;$$
P(AB) = P(A) \cdot P(B)
$$&lt;/p&gt;
&lt;p&gt;하지만 이렇게 도출된 값은 실제 데이터를 가공해 구한 $P(AB)$ 와 차이를 가질 것이다. 이렇게 구한 expected frequency 와 실제 관측된 frequency 간 차이가 클때, 해당 변수는 타겟 변수에 대해 높은 종속성을 가지며, 유용한 feature 라고 판단할 수 있게 되는 것.&lt;/p&gt;
&lt;p&gt;(분포를 직접 비교하는 것이 아니다. 변수 간 독립성 가설이 기각되는지 여부에 따라 종속성을 판단하는 과정이라고 생각하는 것이 보다 정확하다)&lt;/p&gt;
&lt;p&gt;$$
\Chi_c^2 = \sum \frac{(O_i - E_i)^2}{E_i}
$$&lt;/p&gt;
&lt;p&gt;위 방정식에서 $O_i$ 은 observed value, $E_i$ 은 expected value 를 의미한다.&lt;/p&gt;
&lt;p&gt;카이제곱검정 활용에는 다음과 같은 제약 사항이 존재한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;모든 변수가 카테고리 변수일 것&lt;/li&gt;
&lt;li&gt;독립적으로 샘플링 되었을 것&lt;/li&gt;
&lt;li&gt;모든 값에 대한 expected frequency 가 5 이상일 것&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SelectKBest&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chi2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#convert to categorical data by converting data to integers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_cat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#three features with highest chi-squared stats selected&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;chi2_features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SelectKBest&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chi2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_kbest_features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chi2_features&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;fishers-score&#34;&gt;Fisher&amp;rsquo;s Score&lt;/h3&gt;
&lt;p&gt;전통적인 Feature Extraction 방법론이다. ANOVA 와 유사하게 타겟 카테고리 별 분산과, 전체 데이터의 분산을 비교해 통계치를 산출하는 방법 - &lt;a class=&#34;link&#34; href=&#34;https://stats.stackexchange.com/questions/277123/fisher-score-feature-selection-implementation#:~:text=The%20score%20of%20the%20i,of%20the%20i%2Dth%20feature.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;참고 글&lt;/a&gt;. 산정된 통계치를 나열해 가장 &amp;ldquo;유의미한&amp;rdquo; feature 를 특정하는 것이 가능하다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;skfeature.function.similarity_based&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fisher_score&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#calculating scores&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ranks&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fisher_score&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fisher_score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#plotting ranks&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;feat_importances&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Series&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;importances&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataframe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataframe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;feat_importances&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;barh&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;teal&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/feature_3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;correlation-coefficient&#34;&gt;Correlation Coefficient&lt;/h3&gt;
&lt;p&gt;변수 간 선형 관계를 측정해 상관성이 높은 feature 를 추리는 방법이다. 타겟 변수와의 상관성을 파악하는 것은 물론, feature 간 상관성 또한 파악할 수 있기 때문에 다중공신성 문제를 사전에 인지하는데에 도움을 줄 수 있다.&lt;/p&gt;
&lt;p&gt;하지만 해당 방법론 또한 분명한 단점들이 존재한다. 대표적으로 비선형 관계성을 파악하지 못한다는 점을 들 수 있는데, 실무적 환경에서 구축하는 모델이 대부분 비선형성 관계를 전제한다는 점을 생각했을때 실제 모델 적용에 부적합한 방법일 가능성이 높다.&lt;/p&gt;
&lt;p&gt;(다만 $R^2$ 와 같은 결정계수를 활용한다면 실제 선형 관계를 가지는 feature 를 특정하는데 도움을 줄 수 있다)&lt;/p&gt;
&lt;p&gt;또 다른 단점은 선형관계 파악에 지표의 연속성이 전제된다는 점이다. 따라서 타겟 변수가 카테고리 변수인 경우, 상관성을 활용한 특성 선별은 다소 부적합할 수 있다. 문제 특성과 변수의 종류에 따라 여러 방법을 혼용해서 사용하는 것이 필요할 수 있고, 이렇듯 여러가지 접근법으로 동일한 변수가 반복적으로 선별되는 경우 해당 변수는 모델 성능에 기여할 확률이 높을 것이다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;seaborn&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#correlation matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;cor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataframe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#plotting heatmap&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;heatmap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;annot&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/feature_6.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;wrapper-기반-방법&#34;&gt;Wrapper 기반 방법&lt;/h2&gt;
&lt;p&gt;실제 데이터의 subset 을 활용해 모델링을 진행하고, 성능 지표에 기반해 가장 높은 성능을 보이는 feature 집합을 특정하는 방식이다. 당연한 이야기이지만, 모델 적용 환경에서 검증된 feature set 을 특정할 수 있는 대신 연산속도가 느리다는 단점을 가진다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/feature_5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;forward-feature-selection&#34;&gt;Forward Feature Selection&lt;/h3&gt;
&lt;p&gt;성능 기여도가 가장 높은 feature 를 시작으로, feature set 을 순차적으로 늘려가는 방식이다. 마치 greedy algorithm 과 같이 주어진 단계에서, 개별 변수의 기여도를 기반으로 의사결정을 내리는 만큼 변수 조합의 시너지 효과가 충분히 반영되지 않을 수 있다.&lt;/p&gt;
&lt;p&gt;또한 모델 정의가 선행되기 때문에 파라미터 튜닝과 병렬로 진행될 경우 많은 자원이 소모될 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;mlxtend.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SequentialFeatureSelector&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LogisticRegression&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;class_weight&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;balanced&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;solver&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;lbfgs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_jobs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;500&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ffs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SequentialFeatureSelector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_features&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;best&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_jobs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ffs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ffs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_feature_names_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;backward-feature-elimination&#34;&gt;Backward Feature Elimination&lt;/h3&gt;
&lt;p&gt;모든 feature set 을 대상으로 모델링을 진행한 후, 순차적으로 기여도가 낮은 feature 를 제외하는 방식이다. 기본적으로 forward feature selection 과 유사한 장단점을 가진다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;mlxtend.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SequentialFeatureSelector&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LogisticRegression&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;class_weight&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;balanced&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;solver&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;lbfgs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_jobs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;500&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ffs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SequentialFeatureSelector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k_features&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;best&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_jobs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ffs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ffs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k_feature_names_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;exhuastive-feature-selection&#34;&gt;Exhuastive Feature Selection&lt;/h3&gt;
&lt;p&gt;가능한 모든 feature 조합을 비교해 가장 성능이 좋은 feature set 을 추리는 방식이며, 관점에 따라 가장 신뢰도가 높은 방법일 수 있으나 연산 과정에 비효율적인 측면이 존재한다. 데이터 규모에 따라 많은 자원이 소모될 수 있기때문에 문제 적용에 적합한지에 대한 충분한 고민이 필요하다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;mlxtend.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ExhaustiveFeatureSelector&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;RandomForestClassifier&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#create the ExhaustiveFeatureSelector object&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;efs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ExhaustiveFeatureSelector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;min_features&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;max_features&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;scoring&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;roc_auc&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#fit the object to the training data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;efs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;efs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#print the selected features&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;selected_features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_train&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;efs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;best_idx_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;selected_features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;recursive-feature-elimination&#34;&gt;Recursive Feature Elimination&lt;/h3&gt;
&lt;p&gt;우선 전체 feature set 을 대상으로 모델링을 진행한 후, correlation coefficient 와 같은 특정한 지표를 기반으로 일정 비중의 feature 를 제외하는 방법을 재귀적으로 반복하게 된다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;RFE&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;rfe&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;RFE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_features_to_select&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;rfe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rfe&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;embedded-방법&#34;&gt;Embedded 방법&lt;/h2&gt;
&lt;p&gt;모델 구조 내 내장된 변수 채택 기법을 뜻한다. 모델이 &amp;ldquo;알아서&amp;rdquo; feature selection 을 수행하는 것 처럼 비춰질 수 있으나, 사실 정석적인 feature selection 으로 얻는 이점을 곱씹어본다면 (예. 학습 시간 단축 등) 모델링 결과에 기반한 별도 feature selection 과정이 동반되어야 한다는 점은 유사하다고 볼 수 있을 것 같다.&lt;/p&gt;
&lt;p&gt;결국 방식에 차이가 있을뿐, 어떠한 feature 가 가장 모델 성능에 기여하는지를 판별하고, 이를 기반으로 데이터를 전처리 하는 과정은 필수적이다! 라고 볼 수 있다.&lt;/p&gt;
&lt;h3 id=&#34;regularization&#34;&gt;Regularization&lt;/h3&gt;
&lt;p&gt;흔히 알려진 Ridge (L2), Lasso (L1), Elastic Net (L1 &amp;amp; L2) 과 같은 회귀식 기반의 regularization 기법이다. 모델 파라미터의 합산값을 손실 함수에 더해, 최소한의 feature 만을 사용한다는 개념이며, 각각 방법론에 따른 행동양식의 차이가 존재한다 - &lt;a class=&#34;link&#34; href=&#34;https://www.geeksforgeeks.org/lasso-vs-ridge-vs-elastic-net-ml/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;참고 글&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;random-forest-importance&#34;&gt;Random Forest Importance&lt;/h3&gt;
&lt;p&gt;랜덤 포레스트 모델링 시, feature 별 성능 기여도를 판단할 수 있는 importance 지수 산정이 가능하다. 이 중 가장 대표적인 것이 MDI (Mean Decrease in Impurity) Importance 지수인데, 해당되는 feature 를 기반으로 데이터가 나뉘어질때 감소하는 impurity 의 평균치라고 이해할 수 있다 - &lt;a class=&#34;link&#34; href=&#34;https://velog.io/@vvakki_/%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8%EC%97%90%EC%84%9C%EC%9D%98-%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84Variable-Importance-3%EA%B0%80%EC%A7%80#:~:text=%EB%9E%9C%EB%8D%A4%20%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8%EB%9E%80%2C%20%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95,%EB%AA%A8%ED%98%95%28Ensemble%20Model%29%EC%9E%85%EB%8B%88%EB%8B%A4.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;참고 글&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;source&#34;&gt;Source&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=eJIp_mgVLwE&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=eJIp_mgVLwE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=wjsNqBmjBuw&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=wjsNqBmjBuw&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Singular Value Decomposition 의 개념 소개</title>
        <link>https://meme2515.github.io/machine_learning/svd/</link>
        <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/svd/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/svd_1.jpg" alt="Featured image of post Singular Value Decomposition 의 개념 소개" /&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://angeloyeo.github.io/2019/08/01/SVD.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;공돌이의 수학정리노트&lt;/strong&gt;&lt;/a&gt; 블로그를 인용하자면, 특이값 분해가 설명하고자 하는 바는 &lt;strong&gt;&amp;ldquo;직교하는 벡터 집합에 대하여, 선형 변환 후에 그 크기는 변하지만 여전히 직교할 수 있게 되는 그 직교 집합은 무엇인가? 그리고 선형 변환 후의 결과는 무엇인가?&amp;rdquo;&lt;/strong&gt; 로 정리할 수 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;시각적인 설명은 덧붙이자면, 왼편의 직교하는 벡터 집합 $V$ 에 대한 $A$ 매트릭스 선형 변환 결과를 오른편의 벡터 집합 $U\Sigma$ 라고 가정.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. 직교성 조건을 만족하지 못하는 경우&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;시각화를 돕기 위해 오른편의 벡터 크기를 의미하는 $\Sigma$ 매트릭스를 제외한 후, 다음과 같은 결과를 만족하는 크기 1 의 벡터 집합 $U$ 을 찾는 과정이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. 직교성 조건을 만족하는 경우&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;즉, 각자 직교하는 벡터 집합 $U, V$ 는 다음과 같은 관계를 가진다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
AV = U\Sigma
$$&lt;/p&gt;
&lt;h2 id=&#34;특이값-분해의-정의&#34;&gt;특이값 분해의 정의&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;임의의 $m \times n$ 차원 행렬 $A$ 를 다음과 같이 분해하는 행렬 분해 방법 중 하나이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
A = U\Sigma V^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;네 행렬 ($A, U, \Sigma, V$) 의 크기와 성질은 다음과 같이 정리할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
A: m \times n \text{ regular matrix} \newline
U: m \times m \text{ orthogonal matrix} \newline
\Sigma: m \times n \text{ diagonal matrix} \newline
V: n \times n \text{ orthogonal matrix}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Orthogonality 란 &lt;strong&gt;컬럼 별 벡터가 모두 서로 직교하는 성질&lt;/strong&gt;을 칭하는데, 소개 섹션에서 언급했듯 이는 매트릭스 $A$ 에 의해 변환되는 두 개 벡터 집합 $U$ 와 $V$ 의 기본 전제이다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이러한 Orthogonal 매트릭스는 또한 다음과 같은 성질을 가진다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
UU^T = U^T U = I
$$&lt;/p&gt;
&lt;p&gt;$$
U^{-1} = U^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;따라서 다음과 같은 유도가 가능한 것.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
AV = U\Sigma \newline
= AVV^T = U\Sigma V^T \newline
= A = U\Sigma V^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Diagonality 란 &lt;strong&gt;행렬의 대각성분을 제외한 나머지 원소의 값이 모두 $0$ 인 경우&lt;/strong&gt;를 뜻한다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Diagonal Matrix 를 $m \times n$ 모양으로 가정했을 때 행렬 크기가 맞지 않는 애매한 경우가 발생할 수 있다. 이럴때 $m &amp;gt; n$ 인 경우 $n \times n$ 행렬 하단에 모든 원소가 $0$ 인 $m - n \times n$ 행렬이 붙어있게 되거나, $m &amp;lt; n$ 인 경우 $m \times m$ 행렬 오른쪽에 모든 원소가 $0$ 인 $n \times n - m$ 행렬이 붙어있는 식.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 3. 특이값 분해 방정식 내 행렬 도식화&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;u-와-v-를-어떻게-찾을까&#34;&gt;U 와 V 를 어떻게 찾을까?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;우선 다음과 같이 행렬 $A$ 와 $A^T$ 를 정의하자.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
A = U\Sigma V^T \newline
$$&lt;/p&gt;
&lt;p&gt;$$
A^T = (U\Sigma V^T)^T = V\Sigma^T U^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이를 활용해 다음과 같은 수식 관계를 찾을 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
AA^T = U\Sigma V^TV\Sigma^T U^T = U\Sigma^2 U^T
$$&lt;/p&gt;
&lt;p&gt;$$
A^T A = V \Sigma^T U^T U\Sigma V^T = V \Sigma^2 V^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;즉, 벡터 집합 $U$ 와 $V$ 는 각각 $AA^T, A^TA$ 행렬에 대한 eigenvector 이며, 이는 eigendecomposition 테크닉을 활용해 풀이가 가능하다. 관련한 컨텐츠는 &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=PFDu9oVAE-g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;3blue1brown&lt;/a&gt; 과 &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=KTKAp9Q3yWg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ritvikmath&lt;/a&gt; 비디오 참고.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_6.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 4. 고유값 분해 (Eigen-decomposition) 방정식&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;특이값-분해의-목적과-활용&#34;&gt;특이값 분해의 목적과 활용&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;특이값 분해의 공식은 다음과 같이 풀어쓰는 것이 가능하다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
A = U\Sigma V^T
$$&lt;/p&gt;
&lt;p&gt;$$
= {
\begin{pmatrix}
| &amp;amp; | &amp;amp; &amp;amp; |  \newline
u_1 &amp;amp; u_2 &amp;amp; \cdots &amp;amp; u_m  \newline
| &amp;amp; | &amp;amp; &amp;amp; |
\end{pmatrix}
\begin{pmatrix}
\sigma_1 &amp;amp; &amp;amp; &amp;amp; &amp;amp; 0  \newline
&amp;amp; \sigma_2 &amp;amp; &amp;amp; &amp;amp; 0  \newline
&amp;amp; &amp;amp; \ddots &amp;amp; &amp;amp; 0  \newline
&amp;amp; &amp;amp; &amp;amp; \sigma_m &amp;amp; 0  \newline
\end{pmatrix}
\begin{pmatrix}
- &amp;amp; v_1^T &amp;amp; -  \newline
- &amp;amp; v_2^T &amp;amp; -  \newline
&amp;amp; \vdots &amp;amp;   \newline
- &amp;amp; v_n^T &amp;amp; -  \newline
\end{pmatrix}
}
$$&lt;/p&gt;
&lt;p&gt;$$
= \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + &amp;hellip; + \sigma_m u_m v_m^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$u_1 v_1^T$ 등의 결과값은 $m \times n$ 행렬이며, &lt;strong&gt;합산을 통해 최종 행렬인 $A$ 에 다다르기 위한 하나의 레이어&lt;/strong&gt; 정도로 생각할 수 있다. $u$ 와 $v$ 는 정규화된 벡터이기 때문에 $u_1 v_1^T$ 의 값은 1 과 -1 사이에 위치한다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;따라서 $u_1 v_1^T$ 에 의해 특정된 레이어의 정보량은 $\sigma_1$ 에 의해 정해지며, 이를 특이값이라 부른다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;레이어 별 정보량은 특이값의 크기에 의해 결정되기 때문에, 특이값 $p$ 개 만을 이용해 행렬 $A$ 를 부분 복원하는 작업이 가능해진다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_7.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 5. 행렬 $A$ 의 부분 복원 과정 도식화&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;이를 가장 직관적으로 설명하는 방법은 이미지의 SVD 복원 과정을 시각화하는 것이다. 하단의 예시는 이미지를 다수의 레이어로 분할 후, 레이어의 누적 합을 단계적으로 시각화 한 것이다. &lt;strong&gt;초반 몇개의 레이어에 주요 정보들이 집중되어 있고, 후반 레이어로 갈수록 마이너한 정보를 담고 있다.&lt;/strong&gt; 이는 $\Sigma$ 행렬에서 특이값 $\sigma_x$ 이 계속해 작아지며, 이에 상응하는 $u_x v_x^T$ 레이어가 가진 정보량이 감소하는 것과 같은 원리.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_8.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 6. 공돌이의 수학정리노트 블로그에 소개된 SVD 복원 예시&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;차원축소-기법과의-연계점&#34;&gt;차원축소 기법과의 연계점&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$A$ 행렬에서 분해된 $U, \Sigma, V$ 행렬 중, &lt;strong&gt;$V$ 행렬의 각 row vector 는 projection axis, 즉 차원 축소를 수행할 축을 특정하며, 이에 상응하는 $\Sigma$ 행렬의 원소는 해당 projection axis 의 정보량을 특정한다&lt;/strong&gt;. 하단 예시에서 파란색으로 표기된 $v_1$ axis 의 정보량은 $\sigma_1 = 12.4$ 와 같은 식.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_9.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 7. $v_1$ 벡터는 $\sigma_1$ 만큼의 정보량을 가지는 projection axis 를 특정한다&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Projection axis 가 특정 되었다면, 해당 &lt;strong&gt;projection axis 에 데이터를 투영했을때 각 데이터가 가지는 값은 행렬곱 $U\Sigma$ 에 의해 결정&lt;/strong&gt;되게 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_10.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 8. 차원 축소 후 데이터의 위치는 $U\Sigma$ 에 의해 결정&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;본격적인 차원 축소를 수행하기 위해서는 먼저 축소 차원 수 $n$ 을 결정한 후, $\Sigma$ 행렬에서 가장 큰 특이값 $n$ 개를 제외한 나머지 값을 $0$ 으로 바꿔주어야 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 9. 차원 축소 수행을 위해서는 가장 작은 특이값 n 개를 0 으로 설정&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://angeloyeo.github.io/2019/08/01/SVD.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://angeloyeo.github.io/2019/08/01/SVD.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=UyAfmAZU_WI&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=UyAfmAZU_WI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=PFDu9oVAE-g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=PFDu9oVAE-g&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=CpD9XlTu3ys&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=CpD9XlTu3ys&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>AdaBoost 를 통한 Boosting 개념 이해</title>
        <link>https://meme2515.github.io/machine_learning/adaboost/</link>
        <pubDate>Sat, 24 Sep 2022 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/adaboost/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/adaboost.png" alt="Featured image of post AdaBoost 를 통한 Boosting 개념 이해" /&gt;&lt;h2 id=&#34;앙상블-학습&#34;&gt;앙상블 학습&lt;/h2&gt;
&lt;p&gt;앙상블 학습이란 트리 계열 모델 뿐만이 아니라 신경망, 회귀식 등 다양에 형태의 모델에 적용할 수 있다. 예측력이 낮지만 연산 부담 또한 적은 여러개의 모델을 조합해, 하나의 복잡한 모델보다 향상된 성능의 아웃풋을 내는 것을 목적으로 하고 있는데, 특히 Kaggle 등의 데이터 대회에서 트리 계열의 앙상블 학습법은 비신경망 모델링 중 최상위권의 성능을 보여주고있다.&lt;/p&gt;
&lt;p&gt;앙상블 학습법은 크게 Bagging, Stacking, Boosting 으로 분류할 수 있으며, 이 중 가장 복잡하지만 성능이 높은 Boosting 알고리즘을 초기 알고리즘인 AdaBoost 를 통해 설명하고자 한다.&lt;/p&gt;
&lt;h3 id=&#34;bagging--stacking&#34;&gt;Bagging &amp;amp; Stacking&lt;/h3&gt;
&lt;p&gt;Bagging 알고리즘은 &lt;strong&gt;homogenous 한 여러 모델의 조합이다&lt;/strong&gt;. 여기서 homogenous 라 함은 모델의 종류가 하나인, 예를 들어 결정 트리만으로 이루어진 앙상블 모델 등을 의미하며, &lt;strong&gt;개별 모델의 학습은 독립적으로 이루어지게 된다&lt;/strong&gt;. 널리 알려진 랜덤 포레스트 알고리즘은 이러한 Bagging 알고리즘에 데이터 샘플링을 적용한 경우이다.&lt;/p&gt;
&lt;p&gt;Stacking 알고리즘은 이와 다르게 &lt;strong&gt;heterogenous, 즉 서로 다른 종류의 모델의 독립적인 조합&lt;/strong&gt;으로 설명할 수 있다. GLM, 신경망, Bagging 앙상블 모델 등을 조합한 모델을 그 예시로 들 수 있으며, 따라서 모델의 의미를 직관적으로 해석하기에 많은 어려움이 따를 수 있다.&lt;/p&gt;
&lt;h3 id=&#34;boosting&#34;&gt;Boosting&lt;/h3&gt;
&lt;p&gt;Boosting 알고리즘은 &lt;strong&gt;개별 모델의 학습이 이전 모델의 성능에 따라 순차적으로 이루어지는&lt;/strong&gt; 앙상블 학습 방식이다. 따라서 각 모델의 학습은 독립적으로 이루어지지 않으며, 대표적인 예시로 AdaBoost, Gradient Boosting 등을 들 수 있다.&lt;/p&gt;
&lt;h2 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;AdaBoost (Adaptive Boosting)&lt;/strong&gt; 은 1995년 학계에 공개된 이진 분류 모델이다. 유사한 트리 기반 모델인 랜덤 포레스트 알고리즘 또한 같은 연도에 공개되었는데, 이 글에서는 유명한 &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=LsK-xG1cLYA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;StatQuest 비디오&lt;/a&gt;를 참고하여 AdaBoost, 랜덤 포레스트 간의 차이점을 기반으로 개념을 소개하고자 한다. 글을 읽기 전 &lt;a class=&#34;link&#34; href=&#34;https://meme2515.github.io/machine_learning/decision_tree/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;결정 트리&lt;/a&gt;, 랜덤 포레스트 등의 개념을 이해하고 있다는 것을 전제로 하고 있다.&lt;/p&gt;
&lt;p&gt;AdaBoost 알고리즘을 랜덤 포레스트와 구분짓는 중요한 포인트는 다음과 같이 정리할 수 있다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;랜덤 포레스트의 각 트리는 그 자체로 하나의 완전한 결정 트리 모델인 반면, AdaBoost의 각 트리는 &lt;strong&gt;Stump 라는, 한 개 특성을 대상으로 한 번의 분류만을 수행하는 weak learner&lt;/strong&gt; 이다.&lt;/li&gt;
&lt;li&gt;랜덤 포레스트 모델은 각 트리에 동일한 가중치를 적용해 최종 분류값를 결정하는 반면, AdaBoost 는 &lt;strong&gt;트리마다 서로 다른 가중치를 적용한다&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;랜덤 포레스트의 트리는 서로의 학습에 영향을 끼치지않는, 독립적인 모델인 반면 AdaBoost의 각 트리는 &lt;strong&gt;이전 모델의 오분류 케이스를 기반으로, 순차적으로 학습된다&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/adaboost_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. Random Forest 와 AdaBoost 의 차이점&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;stump-생성&#34;&gt;Stump 생성&lt;/h3&gt;
&lt;p&gt;결정 트리 한 개 학습 사이클과 동일하다고 생각하면 된다. 우선 &lt;strong&gt;각 특성 마다 지니 불순도와 CART 알고리즘을 기반으로 한 최적의 임곗값을 특정한 후, 특성 별 모델 중 지니 불순도가 가장 낮은 특성/모델을 기반으로 Stump 를 생성하게 된다.&lt;/strong&gt; 한 개 특성을 대상으로 한 번의 분류만을 수행하기 때문에 (1) 키가 176 보다 크다, (2) 몸무게가 50kg 이상이다 등의 아주 기초적인 분류만을 수행하게 된다.&lt;/p&gt;
&lt;h3 id=&#34;amount-of-say&#34;&gt;Amount of Say&lt;/h3&gt;
&lt;p&gt;Stump 를 생성한 직후에는 &lt;strong&gt;앙상블 모델에 해당 Stump 가 기여할 가중치, Amount of Say (AoS) 를 부여하게된다&lt;/strong&gt;. 이러한 개별 모델의 AoS 는 해당 모델의 분류 정확도에 기반하게 되며, 여기서 &lt;strong&gt;분류 정확도란 지니 계수가 아닌 오분류된 데이터 샘플의 가중치 합, Total Error&lt;/strong&gt; 이다. 데이터의 가중치는 AoS 와는 다른 개념이다. Stump 가 분류하기 어려워하는 데이터를 특정한 후, 이에 더욱 큰 가중치를 부여해 이후 학습에서 강조하는 역할을 한다. 모든 데이터의 가중치 합은 항상 1 이어야 하며, 최초 학습 시 모든 데이터는 동일한 가중치를 부여 받는다.&lt;/p&gt;
&lt;p&gt;AoS 를 구하는 수식은 다음과 같이 정리할 수 있으며, 마치 sigmoid 함수를 90도 돌려놓은 듯한 모양을 가지고있다. 따라서 분류 정확도가 낮아지거나, 높아질수록 AoS 값은 극단적으로 변하게된다.&lt;/p&gt;
&lt;p&gt;$$
\text{AoS} = \frac{1}{2} \text{log} (\frac{1 - \text{total error}}{\text{total error}})
$$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/adaboost_2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. Amount of Say&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;오분류된 데이터를 강조한다는 룰에 따라 데이터 별 가중치는 잘못 분류된 경우 증가하고, 제대로 분류된 경우 감소하게 된다.&lt;/p&gt;
&lt;p&gt;오분류된 데이터의 가중치를 증가시키는 경우, 새로운 가중치는 다음과 같이 정의된다.&lt;/p&gt;
&lt;p&gt;$$
\text{새로운 샘플 가중치} = \text{현재 샘플 가중치} \cdot e^{AoS}
$$&lt;/p&gt;
&lt;p&gt;반대로 정분류된 데이터의 가중치를 감소시키는 경우, 새로운 가중치는 다음과 같이 정의된다.&lt;/p&gt;
&lt;p&gt;$$
\text{새로운 샘플 가중치} = \text{현재 샘플 가중치} \cdot e^{-AoS}
$$&lt;/p&gt;
&lt;p&gt;수식의 의미를 해석하자면, &lt;strong&gt;AoS 가 높은 경우엔 (즉 Stump 의 분류 성능이 좋은 경우) 정분류된 데이터는 해결된 문제라고 판단해 더욱 작은 가중치를 부여하고, 특정된 오분류 데이터에 더욱 높은 가중치를 부여해 이후 해결하는 과정&lt;/strong&gt;이라고 볼 수 있다. 또한 이렇게 업데이트 된 데이터 가중치는 정규화를 통해 그 합이 1 이 되도록 유지한다.&lt;/p&gt;
&lt;h3 id=&#34;의사-결정&#34;&gt;의사 결정&lt;/h3&gt;
&lt;p&gt;Amount of Say 와 Stump 의 개념을 이해했다면 AdaBoost 모델이 의사 결정을 내리는 과정을 쉽게 이해할 수 있다. 일정 개수의 Stump 가 생성된 후, &lt;strong&gt;AdaBoost 는 test 케이스에 대해 A 로 분류한 Stump 의 AoS 합과 B 로 분류한 Stump 의 AoS 합을 비교하여 더욱 큰 AoS 를 가진 클래스로 test 케이스를 분류한다&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;랜덤 포레스트와 Bagging 알고리즘의 관계와 같이 AdaBoost 는 Boosting 알고리즘의 한 케이스에 불과하며, 순차적인 학습의 개념을 이해하기 위해 해당 글이 제시한 예시로 보면 좋을 것 같다. 나아가 이후에 LightGBM, XGBoost 와 같은 Gradient Boosting 알고리즘 또한 정리해 볼 예정이다.&lt;/p&gt;
&lt;h2 id=&#34;레퍼런스&#34;&gt;레퍼런스&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=eLt4a8-316E&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=eLt4a8-316E&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=LsK-xG1cLYA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=LsK-xG1cLYA&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>결정 트리 (Decision Tree) 기초 개념</title>
        <link>https://meme2515.github.io/machine_learning/decision_tree/</link>
        <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/decision_tree/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/decision_tree_1.png" alt="Featured image of post 결정 트리 (Decision Tree) 기초 개념" /&gt;&lt;h2 id=&#34;소개&#34;&gt;소개&lt;/h2&gt;
&lt;p&gt;구직 활동 중 한 회사에서 입사제의를 받았다고 가정하자. 개인마다 그 정도에는 차이가 있겠지만, 제안을 수락하기 까지에는 일종의 의사결정 체계가 존재할 것이다. 대표적으로 다음과 같은 질문을 자신에게 던져볼 수 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;나의 배경과 직급에 적당한 보수를 받을 수 있는가?&lt;/li&gt;
&lt;li&gt;출근 위치는 내가 감내할 수 있는 거리 내에 있는가?&lt;/li&gt;
&lt;li&gt;직원 복지제도가 존재하는가?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;질문에 연관성이 있는 데이터를 가지고 있다면 (보수, 통근거리, 복지제도 유무), 다수의 입사제의에 대해 수락(1) 또는 거절(0) 중 하나의 클래스로 제안에 대한 답변을 분류할 수 있는 알고리즘을 만들 수 있다. 이와 같이 일련의 결정 체계를 통해 분류와 회귀 문제를 효율적으로 수행하는 머신러닝 알고리즘을 결정 트리라고 부른다.&lt;/p&gt;
&lt;p&gt;버클리와 스탠포드에서 1977년 개발한 &lt;strong&gt;CART 알고리즘&lt;/strong&gt; (Breiman et al.) 을 그 기반으로 하고있으며, 2010년 후반부터 널리 사용되고있는 &lt;a class=&#34;link&#34; href=&#34;https://lightgbm.readthedocs.io/en/v3.3.2/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LightGBM&lt;/a&gt;, &lt;a class=&#34;link&#34; href=&#34;https://xgboost.readthedocs.io/en/stable/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;XGBoost&lt;/a&gt; 와 같은 앙상블 학습 알고리즘의 기반이기도하다.&lt;/p&gt;
&lt;h2 id=&#34;결정-트리&#34;&gt;결정 트리&lt;/h2&gt;
&lt;p&gt;머신러닝 예시에서 자주 사용되는 Iris 데이터셋을 활용해 모델의 작동방법을 자세히 알아보자. 아래 시각화된 모델은 주어진 붓꽃의 꽃잎 길이를 기반으로 품종을 분류한다. 먼저 첫 노드에서는 꽃잎의 길이 (petal width) 가 0.8 cm 보다 작거나 같은지 확인한 다음, 그렇다면 붓꽃의 품좀을 setosa 클래스로 분류한다.&lt;/p&gt;
&lt;p&gt;만약 꽃잎의 길이가 0.8 cm 보다 클 경우, 모델은 다음 노드로 이동하여 꽃잎 길이가 1.75 cm 보다 작거나 같은지 확인한다. 그렇다면 붓꽃을 versicolor 클래스로, 그렇지 않다면 virginica 클래스로 분류한다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/decision_tree_2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. Sklearn 패키지의 결정 트리 모델 예시&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;노드의 &lt;strong&gt;samples 속성&lt;/strong&gt;은 학습 과정에서 얼마나 많은 훈련 샘플이 적용되었는지를 헤아리고 있다. 예를 들어 위 예시의 경우 총 150 개의 데이터를 기반으로 학습되었으며, setosa 클래스에는 50 개의 데이터가, versicolor 클래스에는 54 개의 데이터가 학습 과정에서 사용되었던 것을 확인할 수 있다.&lt;/p&gt;
&lt;p&gt;이에 반해 &lt;strong&gt;value 속성&lt;/strong&gt;은 노드에 속한 각 클래스 별 데이터의 수를 보여준다. 예를 들어 우측 하단의 virginica 클래스에는 setosa 클래스가 0 개, versicolor 클래스가 1 개, virginica 클래스가 45 개가 분류되었다. 분류 체계가 완벽하지 않음을 뜻하며, 이는 &lt;strong&gt;gini 속성&lt;/strong&gt;, 즉 이후 설명할 지니 불순도와 연계된다.&lt;/p&gt;
&lt;h2 id=&#34;지니-불순도와-엔트로피&#34;&gt;지니 불순도와 엔트로피&lt;/h2&gt;
&lt;h3 id=&#34;지니-불순도-gini-impurity-score&#34;&gt;지니 불순도 (Gini Impurity Score)&lt;/h3&gt;
&lt;p&gt;지니 불순도는 &lt;strong&gt;특정 노드에 얼마나 다양한 클래스가 분포해있는지를 측정하는 성능 지표&lt;/strong&gt;이다. 노드에 속해있는 샘플의 클래스 분포가 작을수록 0 에 가까워지며, $p_{i,k}$ 를 $i$ 번째 노드에 속한 샘플 중 클래스 $k$ 에 속한 샘플의 비율이라고 했을때 노드 $i$ 에 대한 지니 불순도 $G_i$ 는 다음과 같이 정의할 수 있다.&lt;/p&gt;
&lt;p&gt;$$
G_i = 1 - \sum_{k=1}^n p_{i,k}^2
$$&lt;/p&gt;
&lt;h3 id=&#34;엔트로피-entropy&#34;&gt;엔트로피 (Entropy)&lt;/h3&gt;
&lt;p&gt;지니 불순도와 interchangeably 사용되는 개념이며, 본래 열역학의 개념이다 (분자가 안정되고 질서 정연할 수록 엔트로피는 0에 가까워진다). 노드 $i$ 에 대한 엔트로피 $H_i$ 는 다음과 같이 정의된다.&lt;/p&gt;
&lt;p&gt;$$
H_i = - \sum_{k=1, p_{i,k} \neq 0}^n p_{i,k} \cdot log_2(p_{i,k})
$$&lt;/p&gt;
&lt;p&gt;지니 불순도와 엔트로피 간 생성하는 모델에 큰 차이는 없으며, 지니 불순도의 연산속도가 더 빠르기 때문에 일반적으로 트리 기반 모델은 지니 불순도 평가 지표를 사용하고있다. 다만 모델에 차이가 발생하는 경우 엔트로피가 상대적으로 더 균형 잡힌 트리를 만들게된다.&lt;/p&gt;
&lt;p&gt;여기서 드는 의문점은 지니 불순도와 엔트로피 모두 개별적인 노드에 대한 성능 지표라는 점이다. 일반적인 기계학습이란 모델의 단일 성능 지표 (RMSE, Cross Entropy 등) 를 기반으로 오차율을 줄이는 과정을 거치게 되는데, &lt;strong&gt;결정 트리는 학습 과정 시 전체 모델이 아닌 개별 노드의 성능만을 최적화한다&lt;/strong&gt;. 이러한 알고리즘을 &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Greedy_algorithm&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Greedy Algorithm&lt;/a&gt; 이라 칭한다.&lt;/p&gt;
&lt;h2 id=&#34;cart-훈련-알고리즘&#34;&gt;CART 훈련 알고리즘&lt;/h2&gt;
&lt;p&gt;CART (Classification And Regression Tree) 는 데이터에 대한 최적의 의사 결정 기준을 찾기 위해 고안된 알고리즘이다. 개념적으로 CART 알고리즘은 다음과 같은 순서로 수행된다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;훈련 세트를 여러 특성 $k$ 와 임곗값 $t_k$ 의 조합으로 반복해 분리한다 (예. 꽃잎의 길이 &amp;lt;= 2.45 cm).&lt;/li&gt;
&lt;li&gt;매 사이클 마다 나누어진 두 서브셋에 대한 다음 비용 함수를 계산한다. &lt;em&gt;(여기서 $G$ 는 서브셋의 불순도, $m$ 은 서브셋의 샘플 수를 뜻한다)&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
J(k, t_k) = \frac{m_{left}}{m} G_{left} + \frac{m_{right}}{m} G_{right}
$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;가장 작은 비용 함수를 가진 특성과 임곗값 조합으로 데이터를 나눈다.&lt;/li&gt;
&lt;li&gt;요건을 충족할때 까지 동일한 방식을 통해 나누어진 서브셋에 대한 최적의 특성과 임곗값 조합을 찾는다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;설명한바와 같이 CART 알고리즘은 Greedy Algorithm (탐욕적 알고리즘) 이다. 매 단계에서 알고리즘은 주어진 노드에 대한 최적의 특성과 임곗값 조합을 찾을뿐, 그 이후 과정에 대한 고려는 하지 않는다.&lt;/p&gt;
&lt;h2 id=&#34;하이퍼파라미터&#34;&gt;하이퍼파라미터&lt;/h2&gt;
&lt;p&gt;결정 트리는 별다른 데이터 전처리를 필요로하지 않을뿐만 아니라, 별다른 하이퍼파라미터 또한 필요로 하지 않는다. 대표적으로 조절할 수 있는 것은 결정 트리의 깊이 (depth) 인데, 이는 트리의 높이에 해당하는 개념이며 Scikit-learn 패키지는 &lt;code&gt;max_depth&lt;/code&gt; 매개변수를 통해 이를 조절한다. &lt;code&gt;max_depth&lt;/code&gt; 의 값이 낮을수록 모델을 규제하는 효과를 가진다. 이외에 Scikit-learn 패키지 DecisionTreeClassifier 가 가진 매개변수는 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;min_samples_split&lt;/code&gt; : 분할되기 위해 노드가 가져야 하는 최소 샘플 수&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_samples_leaf&lt;/code&gt; : 리프 노드가 가지고 있어야 할 최소 샘플 수&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_leaf_nodes&lt;/code&gt; : 리프 노드의 최대 수&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_features&lt;/code&gt; : 각 노드에서 분할에 사용할 특성의 최대 수&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;회귀-문제-적용&#34;&gt;회귀 문제 적용&lt;/h2&gt;
&lt;p&gt;클래스의 개념에 노드에 속한 샘플의 평균값을 대입하면 결정 트리를 회귀 문제에 또한 적용할 수 있다. 다만 여기서 CART 알고리즘은 훈련 세트를 불순도를 최소화하는 방향으로 분할하는 대신 평균제곱오차 (MSE) 를 최소화하도록 분할하도록 작동한다.&lt;/p&gt;
&lt;p&gt;$$
J(k,t_k) = \frac{m_{left}}{m} MSE_{left} + \frac{m_{right}}{m} MSE_{right}
$$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/decision_tree_3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. 결정 트리를 사용한 회귀 모델 예시&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;참고-자료&#34;&gt;참고 자료&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hands-On Machine Learning with Scikit-Learn, Keras &amp;amp; Tensorflow&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.explorium.ai/blog/the-complete-guide-to-decision-trees/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.explorium.ai/blog/the-complete-guide-to-decision-trees/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Precision, Recall, F1 스코어 등의 모델 평가 방법</title>
        <link>https://meme2515.github.io/machine_learning/performance_measurement/</link>
        <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/performance_measurement/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/performance_1.png" alt="Featured image of post Precision, Recall, F1 스코어 등의 모델 평가 방법" /&gt;&lt;h2 id=&#34;배경&#34;&gt;배경&lt;/h2&gt;
&lt;p&gt;모델 평가 방법에 대한 사전지식이 없는 누군가에게 스팸 필터 모델에 대한 평가를 요구한다면 아마 정확도 (accuracy) 를 평가 기준으로 선택할 것이다. 정확도는 직관적으로 다음과 같이 정의할 수 있다.&lt;/p&gt;
&lt;p&gt;$$
\text{Accuracy} = \frac{\text{Number of correct labels}}{\text{Number of all cases}}
$$&lt;/p&gt;
&lt;p&gt;경우에 따라 정확도는 적절한 평가 지표가 될 수 있겠지만, 문제가 될 여지 또한 존재한다. 예를 들어 데이터셋에 90가지의 비스팸 메일과, 10가지의 스팸메일이 존재한다고 가정한다면, 별도의 수학적 계산 없이 무조건 메일을 비스팸으로 정의하는 더미 모델은 앞서 정의한 정확도가 90% 에 이르게 된다. 따라서 이 경우에 정확도는 모델의 성능 평가라는 목적에 부합하지 않는 지표이다.&lt;/p&gt;
&lt;p&gt;다음 글에서는 이러한 &lt;a class=&#34;link&#34; href=&#34;https://machinelearningmastery.com/what-is-imbalanced-classification/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Class Imbalance&lt;/a&gt; 문제를 해결하기 위해 고안된 기타 평가 지표들을 설명하고있다.&lt;/p&gt;
&lt;h2 id=&#34;confusion-matrix&#34;&gt;Confusion Matrix&lt;/h2&gt;
&lt;p&gt;평가 지표 개념을 설명하기 전에 &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Confusion_matrix&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;오차 행렬 (Confusion Matrix)&lt;/a&gt; 의 개념을 짚고가자. 기본적으로 오차 행렬은 문제 내 존재하는 클래스들의 예측 조합을 보여준다. 예를 들자면 90건의 클래스 Non-Spam 이 Non-Spam 으로 예측된 경우가 82건, Spam 으로 예측된 경우가 8건과 같은 식이다. 아래 그림을 확인하자.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/performance_1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. 단순 OX 문제에 대한 오차 행렬&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;위 그림에서 Positive(1)이 스팸메일을 뜻할 경우 다음과 같은 네가지 경우의 수가 존재한다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;True Positive (TP)&lt;/strong&gt;: 실제 스팸 메일이 스팸 메일로 올바르게 예측된 경우&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;False Positive (FP)&lt;/strong&gt;: 실제 비스팸 메일이 스팸 메일로 잘못 예측된 경우&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;False Negative (FN)&lt;/strong&gt;: 실제 스팸 메일이 비스팸 메일로 잘못 예측된 경우&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;True Negative (TN)&lt;/strong&gt;: 실제 비스팸 메일이 비스팸 메일로 올바르게 예측된 경우&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;이와 같은 오차 행렬의 언어를 사용하면 Accuracy 지표를 다음과 같이 정의할 수 있게된다.&lt;/p&gt;
&lt;p&gt;$$
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
$$&lt;/p&gt;
&lt;p&gt;경우의 수가 세분화 되었으니, 유사한 방법으로 성능 평가 지표에 대한 다양한 접근이 가능해졌다. 다음 부분에서는 대표적 대안 지표인 Precision 과 Recall 의 정의를 살펴보자.&lt;/p&gt;
&lt;h2 id=&#34;precision--recall&#34;&gt;Precision &amp;amp; Recall&lt;/h2&gt;
&lt;h3 id=&#34;precision&#34;&gt;Precision&lt;/h3&gt;
&lt;p&gt;Precision 이란 다음과 같이 정의할 수 있다.&lt;/p&gt;
&lt;p&gt;$$
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
$$&lt;/p&gt;
&lt;p&gt;즉, 기존 예시에서 &lt;strong&gt;스팸메일로 예측되었던 메일 중 실제 스팸메일의 비율&lt;/strong&gt;을 나타내는 지표이다. Precision 은 예측이 이미 이루어진 상황에서 예측값의 불순도를 측정하며, 무조건적으로 메일을 비스팸으로 분류하는 더미 모델의 경우 10% 의 Precision Score를 가지게 된다. &lt;em&gt;(여기서 positive(1) 값을 스팸으로 정의하는 것이 중요하다. 스팸 메일과 같은 minority class로 positive(1) 값을 설정해야 class imbalance 문제를 해결할 수 있다).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Precision 이 중요한 지표로 작용하는 예시로는 신선한 야채를 골라내는 분류기가 있다. 골라낸 야채 중 상하고 오래된 야채의 비중이 높을수록 판매자는 여러 심각한 리스크를 떠안게 된다. 신선한 야채를 몇개 버릴지언정 상한 야채를 신선한 야채로 분류하는 비율은 최소한으로 유지해야한다.&lt;/p&gt;
&lt;h3 id=&#34;recall&#34;&gt;Recall&lt;/h3&gt;
&lt;p&gt;Recall 이란 다음과 같이 정의할 수 있다.&lt;/p&gt;
&lt;p&gt;$$
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
$$&lt;/p&gt;
&lt;p&gt;Recall 은 &lt;strong&gt;실제 스팸메일 중 스팸메일로 예측된 메일의 비율&lt;/strong&gt;을 나타내는 지표이다. Recall 스코어는 예측이 이루어지기 전 실제 수치와 예측값의 유사도를 측정하며, 더미 모델의 경우 0% 의 Recall Score를 가지게 된다.&lt;/p&gt;
&lt;p&gt;Recall 이 중요한 지표로 작용하는 예시로는 의료적 진단이 있다. 실제 암환자에게 정확한 진단을 내리지 못하는 경우가 많아질수록 환자가 치료시기를 놓칠 위험이 증가하게 된다. 아프지 않은 환자에게 암 진단을 내리는 경우가 생길지언정 실제 암 환자에게 암 진단을 내리지 못하는 비율은 최소한으로 유지해야한다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/performance_3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. Precision Recall 개념의 이해를 돕는 그림&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;f1-score&#34;&gt;F1 Score&lt;/h3&gt;
&lt;p&gt;Precision 과 Recall 을 F1 Score 라는 하나의 지표로 통일하는 방법 또한 존재한다.&lt;/p&gt;
&lt;p&gt;$$
\text{F1 Score} = 2 \cdot \frac{\text{Recall} \cdot \text{Precision}}{\text{Recall} + \text{Precision}}
$$&lt;/p&gt;
&lt;p&gt;Precision 과 Recall 간 &lt;a class=&#34;link&#34; href=&#34;https://wikidocs.net/23088&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;조화평균 (Harmonic Mean)&lt;/a&gt; 값을 구하는 것인데, 산술평균이나 기하평균이 아닌 조화평균을 사용하는 이유는 Precision 과 Recall 간 분모값 차이로 인한 스케일 차이가 발생하기 때문이다. &lt;a class=&#34;link&#34; href=&#34;https://stackoverflow.com/questions/26355942/why-is-the-f-measure-a-harmonic-mean-and-not-an-arithmetic-mean-of-the-precision&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;참고 설명&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;tpr-fpr&#34;&gt;TPR, FPR&lt;/h3&gt;
&lt;p&gt;TPR (True Positive Rate) 의 정의는 다음과 같으며, Recall 의 정의와 동일하다. 따라서 &lt;strong&gt;실제 스팸메일 중 스팸메일로 올바르게 예측된 메일의 비율&lt;/strong&gt; 을 측정한다.&lt;/p&gt;
&lt;p&gt;$$
TPR = \frac{TP}{TP + FN}
$$&lt;/p&gt;
&lt;p&gt;같은 지표가 TPR 이라는 또 다른 이름은 가지는 이유는 FPR (False Positive Rate) 의 개념과 대비하기 위해서다. FPR 은 다음과 같이 정의되며, &lt;strong&gt;실제 비스팸메일 중 스팸메일로 잘못 예측된 메일의 비율&lt;/strong&gt; 을 측정한다.&lt;/p&gt;
&lt;p&gt;$$
FPR = \frac{FP}{FP + TN}
$$&lt;/p&gt;
&lt;h3 id=&#34;sensitivity-specificity&#34;&gt;Sensitivity, Specificity&lt;/h3&gt;
&lt;p&gt;의료 분야에서 주로 사용되는 지표인 Sensitivity 또한 TPR, Recall 의 정의와 동일하며, &lt;strong&gt;실제 스팸메일 중 스팸메일로 올바르게 예측된 메일의 비율&lt;/strong&gt; 을 측정한다.&lt;/p&gt;
&lt;p&gt;$$
\text{Sensitivity} = \frac{TP}{TP + FN}
$$&lt;/p&gt;
&lt;p&gt;Sensitivity 는 Specificity 의 다음 정의와 대비되며, &lt;strong&gt;실제 비스팸메일 중 비스팸메일로 올바르게 예측된 메일의 비율&lt;/strong&gt; 을 측정한다. 즉, FPR 이 비스팸메일 데이터의 오류에 대한 비율이라면 Sensitivity 는 정확도에 대한 비율이라고 이해하면 된다. 같은 분모를 가지고 있지만 다른 분자를 가지고 있는 것을 확인할 수 있다.&lt;/p&gt;
&lt;p&gt;$$
\text{Specificity} = \frac{TN}{FP + TN}
$$&lt;/p&gt;
&lt;h2 id=&#34;pr-curve-roc-curve&#34;&gt;PR Curve, ROC Curve&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/performance_5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 3. 분류기 모델의 ROC, PR Curve 예시&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;precision-recall-pr-curve&#34;&gt;Precision-Recall (PR) Curve&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;신선한 야채를 몇개 버릴지언정&amp;rdquo;, &amp;ldquo;아프지 않은 환자에게 암 진단을 내리는 경우가 생길지언정&amp;rdquo; 과 같은 말은 이 두개 지표 사이에 trade-off 관계가 있음을 암시한다.&lt;/p&gt;
&lt;p&gt;더미 모델이 아닌 실제 각 클래스에 속할 확률을 구하는 모델의 경우, &lt;strong&gt;확률이 몇퍼센트 이상일때 positive(1) 으로 분류할 것인가를 정의하는 threshold 파라미터&lt;/strong&gt;를 가지고 있게된다 &lt;em&gt;(30% 이상의 확률일때 스팸으로 분류, 50% 이상의 확률일때 스팸으로 분류 등)&lt;/em&gt;. 이 threshold 를 움직임에 따라 Precision Recall 지표값이 어떠한 상관관계를 가지고 있는지를 나타내는 그래프를 &lt;strong&gt;Precision-Recall Curve, 혹은 PR Curve&lt;/strong&gt; 라 칭한다.&lt;/p&gt;
&lt;p&gt;위의 예시와 같이 일반적인 분류기는 Precision 이 상승하면 Recall 이 하락하고, Recall 이 상승하면 Precision 이 하락하는 관계를 가지고 있다.&lt;/p&gt;
&lt;h3 id=&#34;roc-curve&#34;&gt;ROC Curve&lt;/h3&gt;
&lt;p&gt;Receiver Operating Characteristic (ROC) Curve 또한 동일하게 threshold 의 움직임에 따라 TPR, FPR 지표의 상관관계를 나타내는 그래프이다. PR Curve 와는 반대로 하나의 지표가 상승할때 다른 하나의 지표 또한 같이 상승하는 관계를 가지고 있으며, 이는 TPR 은 정확도에 대한 지표인 반면 FPR 은 오류율에 대한 지표이기 때문이라고 이해하면 된다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;이상적인 모델은 ROC Curve 의 좌상단에 위치한, 즉 1의 TPR과 0의 FPR을 가지고 있는 모델이다&lt;/strong&gt;. 이는 스팸메일은 항상 스팸메일로, 비스팸메일은 항상 비스팸메일로 분류하는 모델을 뜻하기 때문이다.&lt;/p&gt;
&lt;h3 id=&#34;area-under-the-curve&#34;&gt;Area Under the Curve&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Area Under the Curve (AUC)&lt;/strong&gt; 는 말 그대로 적분을 통해 &lt;strong&gt;PR Curve 와 ROC Curve 의 부피&lt;/strong&gt;를 구한 값이다. 어떤 그래프의 부피인가에 따라 ROC-AUC, PR-AUC 로 정의되며, 모델 평가에 가장 일반적으로 쓰이는 지표는 ROC-AUC 이다. AUC 는 (0, 1) 의 범위를 가지고 있기 떄문에 &lt;strong&gt;ROC-AUC, PR-AUC 모두 1에 가까울수록 정확도가 높은 분류기로 정의할 수 있다&lt;/strong&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/performance_6.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 4. 분류기 모델의 ROC-AUC 예시&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.floydhub.com/a-pirates-guide-to-accuracy-precision-recall-and-other-scores/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.floydhub.com/a-pirates-guide-to-accuracy-precision-recall-and-other-scores/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://uberpython.wordpress.com/2012/01/01/precision-recall-sensitivity-and-specificity/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://uberpython.wordpress.com/2012/01/01/precision-recall-sensitivity-and-specificity/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hands-On Machine Learning with Scikit-learn, Keras and Tersorflow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Conda 환경 공유 방법</title>
        <link>https://meme2515.github.io/machine_learning/conda_1/</link>
        <pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/conda_1/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/conda.png" alt="Featured image of post Conda 환경 공유 방법" /&gt;&lt;h2 id=&#34;배경&#34;&gt;배경&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.conda.io/en/latest/#&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;콘다&lt;/a&gt;는 윈도우, 맥OS, 리눅스에서 동작하는 패키지 관리 시스템이며, 데이터 분석 환경에서 주로 사용되지만 파이썬, R, 루비, 자바 등 다양한 언어를 지원한다. 본 글에서는 짧게 콘다 환경 생성과 세팅, 저장, 그리고 다른 컴퓨터에서 저장된 환경을 불러오는 법을 살펴보고자 한다.&lt;/p&gt;
&lt;h2 id=&#34;환경-생성-및-세팅-저장&#34;&gt;환경 생성 및 세팅, 저장&lt;/h2&gt;
&lt;h3 id=&#34;생성-및-패키지-설치&#34;&gt;생성 및 패키지 설치&lt;/h3&gt;
&lt;p&gt;Conda 환경은 다음과 같이 생성할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; conda create --name [환경이름] python=3.10
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;생성된 모든 conda 환경은 다음 커맨드로 확인할 수 있다. &lt;code&gt;*&lt;/code&gt; 표시는 현재 환경을 나타낸다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; conda env list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &amp;gt;&amp;gt;&amp;gt; conda environments:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     base                       *
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     environment1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     environment2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;환경을 바꾸기 위해서는 &lt;code&gt;activate&lt;/code&gt; 커맨드를 사용한다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; conda activate environment1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; conda env list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &amp;gt;&amp;gt;&amp;gt; conda environments:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     base
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     environment1               *
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     environment2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;현재 환경에 설치된 패키지는 다음과 같이 확인할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; conda list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &amp;gt;&amp;gt;&amp;gt; numba             0.48.0              py37h47e9c7a_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     numpy             1.18.1              py37h93ca92e_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     openssl           1.1.1d                  he774522_4
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     pandas            1.0.1               py37h47e9c7a_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;패키지를 설치하기 위해서는 주로 &lt;code&gt;pip install&lt;/code&gt;, 혹은 &lt;code&gt;conda install&lt;/code&gt; 커맨드를 사용하게 된다. &lt;a class=&#34;link&#34; href=&#34;https://pypi.org/project/pip/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;pip&lt;/a&gt;은 파이썬 전용 패키지인 반면, conda는 기타 언어의 패키지 관리를 지원한다는 차이점을 가지고있다. 다음 예시는 pip 패키지 매니저를 활용했다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; pip install cython
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; conda list
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &amp;gt;&amp;gt;&amp;gt; cython            0.29.15             py37ha925a31_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     numba             0.48.0              py37h47e9c7a_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     numpy             1.18.1              py37h93ca92e_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     openssl           1.1.1d                  he774522_4
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     pandas            1.0.1               py37h47e9c7a_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;yaml-파일-저장&#34;&gt;YAML 파일 저장&lt;/h3&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.redhat.com/en/topics/automation/what-is-yaml&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;YAML&lt;/a&gt; 포맷으로 환경 설정을 저장하기 위해서는 다음 커맨드를 활용한다. YAML 파일명은 굳이 환경 이름과 매칭되지 않아도 괜찮다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; conda env export &amp;gt; environment1.yaml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;이후 해당 커맨드를 실행한 경로에 environment1.yaml 이라는 파일이 생성되게 된다. 해당 파일을 열어보면 다음과 같이 설치된 패키지가 나열되어 있는것을 확인할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; name: environment1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; channels:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   - conda_forge
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   - defaults
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; dependencies:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   - cython=0.29.15=py37ha925a31_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   - numba=0.48.0=py37h47e9c7a_0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;yaml-파일을-활용한-환경-생성&#34;&gt;YAML 파일을 활용한 환경 생성&lt;/h2&gt;
&lt;p&gt;다른 컴퓨터에서 저장된 conda 환경과 동일한 환경을 생성하고자 할때, 커맨드창에서 YAML 파일 경로로 이동 후 다음을 실행시키면 된다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; conda env create --file environment1.yaml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        
    </channel>
</rss>
