<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>T5 on Soon&#39;s Blog</title>
        <link>https://meme2515.github.io/categories/t5/</link>
        <description>Recent content in T5 on Soon&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 12 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://meme2515.github.io/categories/t5/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>T5 (Text-To-Text Transfer Tranformer) 구조 소개</title>
        <link>https://meme2515.github.io/neural_network/t5/</link>
        <pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/neural_network/t5/</guid>
        <description>&lt;img src="https://meme2515.github.io/neural_network/images/t5_1.gif" alt="Featured image of post T5 (Text-To-Text Transfer Tranformer) 구조 소개" /&gt;&lt;ul&gt;
&lt;li&gt;본 글은 T5 구조에 대한 Google AI 의 공식적인 &lt;a class=&#34;link&#34; href=&#34;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;소개글&lt;/a&gt;을 번역한 것이며, 실제 논문은 &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1910.10683&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;본 링크&lt;/a&gt;를 참조할 것.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;h4 id=&#34;background&#34;&gt;Background&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;최근 NLP 분야의 성장은 웹에서 확보할 수 있는 수많은 unlabeled 텍스트를 활용한 전이 학습의 효율성에 기인.&lt;/li&gt;
&lt;li&gt;이러한 학습은 주로 self-supervised 형식으로 이루어지며, 빈칸 채우기 등의 태스크를 수행하는 것을 1차적인 과제로 삼음.&lt;/li&gt;
&lt;li&gt;방대한 데이터를 기반으로 사전 학습을 마친 모델은 별도 데이터를 활용해 finetune 될 수 있으며, 이는 보유한 데이터만으로 모델을 학습하는 것 보다 월등히 나은 성능을 보임.&lt;/li&gt;
&lt;li&gt;2018 년 부터 &lt;strong&gt;GPT, ULMFit, ELMo, BERT&lt;/strong&gt; 등 다양한 전이학습의 성공 사례가 보고되었으며, 2019 년도에는 &lt;strong&gt;XLNet, RoBERTa, ALBERT, Reformer, MT-DNN&lt;/strong&gt; 등 보다 개선된 방식이 개발.&lt;/li&gt;
&lt;li&gt;발전 속도가 워낙 빠르기 때문에, 어떠한 개선점이 유효하고, 어떠한 모델의 조합이 효과적인지를 판단하기 어려운 면이 존재한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;t5&#34;&gt;T5&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;본 논문에서는 가장 효과적인 전이 학습 방식을 평가하기 위한 비교 실험을 진행하였으며, 결과를 기반으로 T5 모델을 구축.&lt;/li&gt;
&lt;li&gt;또한 새로운 사전 학습 데이터셋인 &lt;a class=&#34;link&#34; href=&#34;https://www.tensorflow.org/datasets/catalog/c4&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Colossal Clean Crawled Corpus (C4)&lt;/a&gt; 를 공개했다.&lt;/li&gt;
&lt;li&gt;C4 데이터를 기반으로 학습된 T5 모델은 공개 당시 state-of-the-art 성능을 기록했으며, 또한 다양한 태스크에 접목될 수 있는 유연성을 가지고 있다.&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/google-research/text-to-text-transfer-transformer&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;코드&lt;/a&gt;. &lt;a class=&#34;link&#34; href=&#34;https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;사전 학습 모델&lt;/a&gt;. &lt;a class=&#34;link&#34; href=&#34;https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Colab Notebook&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;a-shared-text-to-text-framework&#34;&gt;A Shared Text-To-Text Framework&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;T5 를 정의하는 핵심적인 요소는 모든 NLP 태스크를 text-to-text 포맷으로 통일시켰다는 점. 이는 BERT 처럼 클래스 레이블 등으로 한정된 아웃풋을 출력하는 모델과 차별되는 포인트이다.&lt;/li&gt;
&lt;li&gt;text-to-text 프레임워크는 동일한 모델, 손실 함수, 하이퍼파라미터를 활용해 모든 NLP 태스크 수행이 가능하며, 이는 기계 번역, 문서 요약, 질답, 분류, 회귀 (출력값을 텍스트로 변환) 등의 태스크를 포함한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/neural_network/images/t5_1.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. Diagram of text-to-text framework&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;a-large-pre-training-dataset-c4&#34;&gt;A Large Pre-training Dataset (C4)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;전이 학습의 핵심적인 요소는 사전 학습에 사용되는 unlabeled 데이터셋이다.&lt;/li&gt;
&lt;li&gt;사전 학습 수준에 따른 성능 편차를 정확하게 측정하기 위해서는 질이 높고, 다양성이 높으며, 규모가 큰 데이터를 필요로 하는데 연구진은 이러한 조건을 모두 충족하는 데이터셋이 존재하지 않는다고 판단.&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.wikipedia.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;위키피디아&lt;/a&gt; 데이터셋은 품질이 높지만, 규모가 크지 않으며 스타일이 획일적. &lt;a class=&#34;link&#34; href=&#34;https://commoncrawl.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Common Crawl&lt;/a&gt; 데이터셋은 규모는 크지만 데이터의 품질이 낮다.&lt;/li&gt;
&lt;li&gt;이러한 조건을 모두 충족하는 데이터셋을 구축하기 위해 연구진은 기존 Common Crawl 데이터셋을 정제한 C4 데이터셋을 구축하였으며, 이는 위키피디아 데이터셋에 비해 약 100배의 규모를 가지고 있다.&lt;/li&gt;
&lt;li&gt;적용된 정제 과정은 완성되지 않은 문장 제외, 중복 데이터 제외, offensive 혹은 noisy 한 데이터 제외 등.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;a-systematic-study-of-transfer-learning-methodology&#34;&gt;A Systematic Study of Transfer Learning Methodology&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;연구진은 상기된 T5 프레임워크와 C4 데이터셋을 활용해 알려진 다양한 NLP 전이 학습 방법을 비교했다.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Model Architectures&lt;/strong&gt; : Encoder-Decoder 모델 구조는 대체로 Decoder-Only 구조에 비해 높은 성능을 보임.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pre-training Objectives&lt;/strong&gt; : Fill-in-the-blank 스타일의 학습 목표가 가장 유연한 모델을 생성.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unlabeled Datasets&lt;/strong&gt; : 최종 목적과 부합한 in-domain 데이터의 사전 학습은 도움이 되었으나, 사전 학습 데이터셋이 너무 작은 경우 over-fitting 문제 발생.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training Strategies&lt;/strong&gt; : 멀티태스트 러닝은 사전 학습 후 전이 학습을 진행하는 방식과 유사한 성능을 낼 수 있지만, 각 태스크에 따른 학습 주기를 세밀하게 조정해야 함.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt; : 한정된 연산 자원 배분을 위해 적합한 모델 사이즈, 학습 시간, 앙상블 모델 수 등을 탐색.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;insights--scale--state-of-the-art&#34;&gt;Insights + Scale = State-of-the-Art&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;비교 분석을 통해 얻은 인사이트를 기반으로, TPU 를 활용한 모델 스케일링을 진행. 최종 모델은 약 11 billion (110 억) 개의 파라미터를 가지고 있다.&lt;/li&gt;
&lt;li&gt;GLUE, SuperGLUE, SQuAD, CNN/Daily Mail 벤치마크 등에서 당시 가장 높은 성능을 기록.&lt;/li&gt;
&lt;li&gt;특히 주목할 점은 SuperGLUE 에서 인간과 유사한 점수를 기록했다는 것인데, 해당 데이터셋은 고의적으로 머신러닝으로 해결하기 어려운 데이터를 주로 포함.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;extensions&#34;&gt;Extensions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;T5 는 논문에 언급된 것 이외에도 많은 태스크에 쉽게 적용할 수 있다는 장점을 가지고 있다.&lt;/li&gt;
&lt;li&gt;다음 섹션에서는 Closed-Book Question Answering 과 변측적인 blank-size 에 대한 fill-in-the-blank 과제 적용 사례를 설명.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;closed-book-question-answering&#34;&gt;Closed-Book Question Answering&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;지문과 질문을 인풋으로 받았을 때, 질문에 대한 적절한 답변을 생성하는 과제.&lt;/li&gt;
&lt;li&gt;예시적으로 Hurricane Connie 에 대한 위키피디아 지문과 Hurricane Connie 는 언제 발생했는가? 라는 질문을 받았을때 모델은 &amp;ldquo;August 3rd, 1955&amp;rdquo; 와 같이 적절한 답변을 아웃풋해야 한다.&lt;/li&gt;
&lt;li&gt;이러한 과제를 위해 설계된 Stanford Question Answering Dataset (SQuAD) 에서 T5 는 당시 가장 높은 성능을 기록.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/neural_network/images/t5_2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. T5 learns to fill in dropped-out spans of text&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Colab 데모와 논문에서 연구진은 컨텍스트 없이 모델이 적절한 답변을 할 수 있는지를 테스트 하였으며, TriviaQA, WebQuestions, Natural Questions 데이터셋에서 원문 그대로의 답변을 제시한 비율이 각각 50.1%, 37.4%, 34.5% 를 기록했다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/neural_network/images/t5_3.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 3. Question answering UI&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;fill-in-the-blank-text-generation&#34;&gt;Fill-in-the-Blank Text Generation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;GPT-2 와 같은 LLM 은 실제 사람이 작성한 것과 유사한 텍스트를 생성하는 작업에 매우 탁월한 성능을 보인다. 이는 &lt;a class=&#34;link&#34; href=&#34;https://app.inferkit.com/demo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Talk To Transformer&lt;/a&gt; 와 &lt;a class=&#34;link&#34; href=&#34;https://aidungeon.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;AI Dungeon&lt;/a&gt; 과 같은 적용 사례로 까지 이어짐.&lt;/li&gt;
&lt;li&gt;이는 fill-in-the-blank 과제에서 빈칸이 가장 뒷편에 있는 경우라고 해석할 수 있으며, 해당 과제는 T5 의 사전 학습 과제와 일치함.&lt;/li&gt;
&lt;li&gt;연구진은 빈칸에 들어갈 단어 수를 제시하고, 모델이 이를 채워넣는 과제를 실험하였으며 사실적으로 생성된 텍스트를 확인함.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
