<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>machine learning on Soon&#39;s Blog</title>
        <link>https://meme2515.github.io/categories/machine-learning/</link>
        <description>Recent content in machine learning on Soon&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 30 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://meme2515.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Singular Value Decomposition 의 개념 소개</title>
        <link>https://meme2515.github.io/machine_learning/svd/</link>
        <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/svd/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/svd_1.jpg" alt="Featured image of post Singular Value Decomposition 의 개념 소개" /&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://angeloyeo.github.io/2019/08/01/SVD.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;공돌이의 수학정리노트&lt;/strong&gt;&lt;/a&gt; 블로그를 인용하자면, 특이값 분해가 설명하고자 하는 바는 &lt;strong&gt;&amp;ldquo;직교하는 벡터 집합에 대하여, 선형 변환 후에 그 크기는 변하지만 여전히 직교할 수 있게 되는 그 직교 집합은 무엇인가? 그리고 선형 변환 후의 결과는 무엇인가?&amp;rdquo;&lt;/strong&gt; 로 정리할 수 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;시각적인 설명은 덧붙이자면, 왼편의 직교하는 벡터 집합 $V$ 에 대한 $A$ 매트릭스 선형 변환 결과를 오른편의 벡터 집합 $U\Sigma$ 라고 가정.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. 직교성 조건을 만족하지 못하는 경우&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;시각화를 돕기 위해 오른편의 벡터 크기를 의미하는 $\Sigma$ 매트릭스를 제외한 후, 다음과 같은 결과를 만족하는 크기 1 의 벡터 집합 $U$ 을 찾는 과정이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. 직교성 조건을 만족하는 경우&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;즉, 각자 직교하는 벡터 집합 $U, V$ 는 다음과 같은 관계를 가진다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
AV = U\Sigma
$$&lt;/p&gt;
&lt;h2 id=&#34;특이값-분해의-정의&#34;&gt;특이값 분해의 정의&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;임의의 $m \times n$ 차원 행렬 $A$ 를 다음과 같이 분해하는 행렬 분해 방법 중 하나이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
A = U\Sigma V^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;네 행렬 ($A, U, \Sigma, V$) 의 크기와 성질은 다음과 같이 정리할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
A: m \times n \text{ regular matrix} \newline
U: m \times m \text{ orthogonal matrix} \newline
\Sigma: m \times n \text{ diagonal matrix} \newline
V: n \times n \text{ orthogonal matrix}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Orthogonality 란 &lt;strong&gt;컬럼 별 벡터가 모두 서로 직교하는 성질&lt;/strong&gt;을 칭하는데, 소개 섹션에서 언급했듯 이는 매트릭스 $A$ 에 의해 변환되는 두 개 벡터 집합 $U$ 와 $V$ 의 기본 전제이다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이러한 Orthogonal 매트릭스는 또한 다음과 같은 성질을 가진다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
UU^T = U^T U = I
$$&lt;/p&gt;
&lt;p&gt;$$
U^{-1} = U^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;따라서 다음과 같은 유도가 가능한 것.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
AV = U\Sigma \newline
= AVV^T = U\Sigma V^T \newline
= A = U\Sigma V^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Diagonality 란 &lt;strong&gt;행렬의 대각성분을 제외한 나머지 원소의 값이 모두 $0$ 인 경우&lt;/strong&gt;를 뜻한다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Diagonal Matrix 를 $m \times n$ 모양으로 가정했을 때 행렬 크기가 맞지 않는 애매한 경우가 발생할 수 있다. 이럴때 $m &amp;gt; n$ 인 경우 $n \times n$ 행렬 하단에 모든 원소가 $0$ 인 $m - n \times n$ 행렬이 붙어있게 되거나, $m &amp;lt; n$ 인 경우 $m \times m$ 행렬 오른쪽에 모든 원소가 $0$ 인 $n \times n - m$ 행렬이 붙어있는 식.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 3. 특이값 분해 방정식 내 행렬 도식화&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;u-와-v-를-어떻게-찾을까&#34;&gt;U 와 V 를 어떻게 찾을까?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;우선 다음과 같이 행렬 $A$ 와 $A^T$ 를 정의하자.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
A = U\Sigma V^T \newline
$$&lt;/p&gt;
&lt;p&gt;$$
A^T = (U\Sigma V^T)^T = V\Sigma^T U^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이를 활용해 다음과 같은 수식 관계를 찾을 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
AA^T = U\Sigma V^TV\Sigma^T U^T = U\Sigma^2 U^T
$$&lt;/p&gt;
&lt;p&gt;$$
A^T A = V \Sigma^T U^T U\Sigma V^T = V \Sigma^2 V^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;즉, 벡터 집합 $U$ 와 $V$ 는 각각 $AA^T, A^TA$ 행렬에 대한 eigenvector 이며, 이는 eigendecomposition 테크닉을 활용해 풀이가 가능하다. 관련한 컨텐츠는 &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=PFDu9oVAE-g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;3blue1brown&lt;/a&gt; 과 &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=KTKAp9Q3yWg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ritvikmath&lt;/a&gt; 비디오 참고.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_6.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 4. 고유값 분해 (Eigen-decomposition) 방정식&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;특이값-분해의-목적과-활용&#34;&gt;특이값 분해의 목적과 활용&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;특이값 분해의 공식은 다음과 같이 풀어쓰는 것이 가능하다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
A = U\Sigma V^T
$$&lt;/p&gt;
&lt;p&gt;$$
= {
\begin{pmatrix}
| &amp;amp; | &amp;amp; &amp;amp; |  \newline
u_1 &amp;amp; u_2 &amp;amp; \cdots &amp;amp; u_m  \newline
| &amp;amp; | &amp;amp; &amp;amp; |
\end{pmatrix}
\begin{pmatrix}
\sigma_1 &amp;amp; &amp;amp; &amp;amp; &amp;amp; 0  \newline
&amp;amp; \sigma_2 &amp;amp; &amp;amp; &amp;amp; 0  \newline
&amp;amp; &amp;amp; \ddots &amp;amp; &amp;amp; 0  \newline
&amp;amp; &amp;amp; &amp;amp; \sigma_m &amp;amp; 0  \newline
\end{pmatrix}
\begin{pmatrix}
- &amp;amp; v_1^T &amp;amp; -  \newline
- &amp;amp; v_2^T &amp;amp; -  \newline
&amp;amp; \vdots &amp;amp;   \newline
- &amp;amp; v_n^T &amp;amp; -  \newline
\end{pmatrix}
}
$$&lt;/p&gt;
&lt;p&gt;$$
= \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + &amp;hellip; + \sigma_m u_m v_m^T
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$u_1 v_1^T$ 등의 결과값은 $m \times n$ 행렬이며, &lt;strong&gt;합산을 통해 최종 행렬인 $A$ 에 다다르기 위한 하나의 레이어&lt;/strong&gt; 정도로 생각할 수 있다. $u$ 와 $v$ 는 정규화된 벡터이기 때문에 $u_1 v_1^T$ 의 값은 1 과 -1 사이에 위치한다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;따라서 $u_1 v_1^T$ 에 의해 특정된 레이어의 정보량은 $\sigma_1$ 에 의해 정해지며, 이를 특이값이라 부른다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;레이어 별 정보량은 특이값의 크기에 의해 결정되기 때문에, 특이값 $p$ 개 만을 이용해 행렬 $A$ 를 부분 복원하는 작업이 가능해진다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_7.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 5. 행렬 $A$ 의 부분 복원 과정 도식화&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;이를 가장 직관적으로 설명하는 방법은 이미지의 SVD 복원 과정을 시각화하는 것이다. 하단의 예시는 이미지를 다수의 레이어로 분할 후, 레이어의 누적 합을 단계적으로 시각화 한 것이다. &lt;strong&gt;초반 몇개의 레이어에 주요 정보들이 집중되어 있고, 후반 레이어로 갈수록 마이너한 정보를 담고 있다.&lt;/strong&gt; 이는 $\Sigma$ 행렬에서 특이값 $\sigma_x$ 이 계속해 작아지며, 이에 상응하는 $u_x v_x^T$ 레이어가 가진 정보량이 감소하는 것과 같은 원리.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_8.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 6. 공돌이의 수학정리노트 블로그에 소개된 SVD 복원 예시&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;차원축소-기법과의-연계점&#34;&gt;차원축소 기법과의 연계점&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$A$ 행렬에서 분해된 $U, \Sigma, V$ 행렬 중, &lt;strong&gt;$V$ 행렬의 각 row vector 는 projection axis, 즉 차원 축소를 수행할 축을 특정하며, 이에 상응하는 $\Sigma$ 행렬의 원소는 해당 projection axis 의 정보량을 특정한다&lt;/strong&gt;. 하단 예시에서 파란색으로 표기된 $v_1$ axis 의 정보량은 $\sigma_1 = 12.4$ 와 같은 식.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_9.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 7. $v_1$ 벡터는 $\sigma_1$ 만큼의 정보량을 가지는 projection axis 를 특정한다&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Projection axis 가 특정 되었다면, 해당 &lt;strong&gt;projection axis 에 데이터를 투영했을때 각 데이터가 가지는 값은 행렬곱 $U\Sigma$ 에 의해 결정&lt;/strong&gt;되게 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_10.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 8. 차원 축소 후 데이터의 위치는 $U\Sigma$ 에 의해 결정&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;본격적인 차원 축소를 수행하기 위해서는 먼저 축소 차원 수 $n$ 을 결정한 후, $\Sigma$ 행렬에서 가장 큰 특이값 $n$ 개를 제외한 나머지 값을 $0$ 으로 바꿔주어야 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/svd_11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 9. 차원 축소 수행을 위해서는 가장 작은 특이값 n 개를 0 으로 설정&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://angeloyeo.github.io/2019/08/01/SVD.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://angeloyeo.github.io/2019/08/01/SVD.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=UyAfmAZU_WI&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=UyAfmAZU_WI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=PFDu9oVAE-g&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=PFDu9oVAE-g&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=CpD9XlTu3ys&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=CpD9XlTu3ys&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>AdaBoost 를 통한 Boosting 개념 이해</title>
        <link>https://meme2515.github.io/machine_learning/adaboost/</link>
        <pubDate>Sat, 24 Sep 2022 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/adaboost/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/adaboost.png" alt="Featured image of post AdaBoost 를 통한 Boosting 개념 이해" /&gt;&lt;h2 id=&#34;앙상블-학습&#34;&gt;앙상블 학습&lt;/h2&gt;
&lt;p&gt;앙상블 학습이란 트리 계열 모델 뿐만이 아니라 신경망, 회귀식 등 다양에 형태의 모델에 적용할 수 있다. 예측력이 낮지만 연산 부담 또한 적은 여러개의 모델을 조합해, 하나의 복잡한 모델보다 향상된 성능의 아웃풋을 내는 것을 목적으로 하고 있는데, 특히 Kaggle 등의 데이터 대회에서 트리 계열의 앙상블 학습법은 비신경망 모델링 중 최상위권의 성능을 보여주고있다.&lt;/p&gt;
&lt;p&gt;앙상블 학습법은 크게 Bagging, Stacking, Boosting 으로 분류할 수 있으며, 이 중 가장 복잡하지만 성능이 높은 Boosting 알고리즘을 초기 알고리즘인 AdaBoost 를 통해 설명하고자 한다.&lt;/p&gt;
&lt;h3 id=&#34;bagging--stacking&#34;&gt;Bagging &amp;amp; Stacking&lt;/h3&gt;
&lt;p&gt;Bagging 알고리즘은 &lt;strong&gt;homogenous 한 여러 모델의 조합이다&lt;/strong&gt;. 여기서 homogenous 라 함은 모델의 종류가 하나인, 예를 들어 결정 트리만으로 이루어진 앙상블 모델 등을 의미하며, &lt;strong&gt;개별 모델의 학습은 독립적으로 이루어지게 된다&lt;/strong&gt;. 널리 알려진 랜덤 포레스트 알고리즘은 이러한 Bagging 알고리즘에 데이터 샘플링을 적용한 경우이다.&lt;/p&gt;
&lt;p&gt;Stacking 알고리즘은 이와 다르게 &lt;strong&gt;heterogenous, 즉 서로 다른 종류의 모델의 독립적인 조합&lt;/strong&gt;으로 설명할 수 있다. GLM, 신경망, Bagging 앙상블 모델 등을 조합한 모델을 그 예시로 들 수 있으며, 따라서 모델의 의미를 직관적으로 해석하기에 많은 어려움이 따를 수 있다.&lt;/p&gt;
&lt;h3 id=&#34;boosting&#34;&gt;Boosting&lt;/h3&gt;
&lt;p&gt;Boosting 알고리즘은 &lt;strong&gt;개별 모델의 학습이 이전 모델의 성능에 따라 순차적으로 이루어지는&lt;/strong&gt; 앙상블 학습 방식이다. 따라서 각 모델의 학습은 독립적으로 이루어지지 않으며, 대표적인 예시로 AdaBoost, Gradient Boosting 등을 들 수 있다.&lt;/p&gt;
&lt;h2 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;AdaBoost (Adaptive Boosting)&lt;/strong&gt; 은 1995년 학계에 공개된 이진 분류 모델이다. 유사한 트리 기반 모델인 랜덤 포레스트 알고리즘 또한 같은 연도에 공개되었는데, 이 글에서는 유명한 &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=LsK-xG1cLYA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;StatQuest 비디오&lt;/a&gt;를 참고하여 AdaBoost, 랜덤 포레스트 간의 차이점을 기반으로 개념을 소개하고자 한다. 글을 읽기 전 &lt;a class=&#34;link&#34; href=&#34;https://meme2515.github.io/machine_learning/decision_tree/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;결정 트리&lt;/a&gt;, 랜덤 포레스트 등의 개념을 이해하고 있다는 것을 전제로 하고 있다.&lt;/p&gt;
&lt;p&gt;AdaBoost 알고리즘을 랜덤 포레스트와 구분짓는 중요한 포인트는 다음과 같이 정리할 수 있다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;랜덤 포레스트의 각 트리는 그 자체로 하나의 완전한 결정 트리 모델인 반면, AdaBoost의 각 트리는 &lt;strong&gt;Stump 라는, 한 개 특성을 대상으로 한 번의 분류만을 수행하는 weak learner&lt;/strong&gt; 이다.&lt;/li&gt;
&lt;li&gt;랜덤 포레스트 모델은 각 트리에 동일한 가중치를 적용해 최종 분류값를 결정하는 반면, AdaBoost 는 &lt;strong&gt;트리마다 서로 다른 가중치를 적용한다&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;랜덤 포레스트의 트리는 서로의 학습에 영향을 끼치지않는, 독립적인 모델인 반면 AdaBoost의 각 트리는 &lt;strong&gt;이전 모델의 오분류 케이스를 기반으로, 순차적으로 학습된다&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/adaboost_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. Random Forest 와 AdaBoost 의 차이점&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;stump-생성&#34;&gt;Stump 생성&lt;/h3&gt;
&lt;p&gt;결정 트리 한 개 학습 사이클과 동일하다고 생각하면 된다. 우선 &lt;strong&gt;각 특성 마다 지니 불순도와 CART 알고리즘을 기반으로 한 최적의 임곗값을 특정한 후, 특성 별 모델 중 지니 불순도가 가장 낮은 특성/모델을 기반으로 Stump 를 생성하게 된다.&lt;/strong&gt; 한 개 특성을 대상으로 한 번의 분류만을 수행하기 때문에 (1) 키가 176 보다 크다, (2) 몸무게가 50kg 이상이다 등의 아주 기초적인 분류만을 수행하게 된다.&lt;/p&gt;
&lt;h3 id=&#34;amount-of-say&#34;&gt;Amount of Say&lt;/h3&gt;
&lt;p&gt;Stump 를 생성한 직후에는 &lt;strong&gt;앙상블 모델에 해당 Stump 가 기여할 가중치, Amount of Say (AoS) 를 부여하게된다&lt;/strong&gt;. 이러한 개별 모델의 AoS 는 해당 모델의 분류 정확도에 기반하게 되며, 여기서 &lt;strong&gt;분류 정확도란 지니 계수가 아닌 오분류된 데이터 샘플의 가중치 합, Total Error&lt;/strong&gt; 이다. 데이터의 가중치는 AoS 와는 다른 개념이다. Stump 가 분류하기 어려워하는 데이터를 특정한 후, 이에 더욱 큰 가중치를 부여해 이후 학습에서 강조하는 역할을 한다. 모든 데이터의 가중치 합은 항상 1 이어야 하며, 최초 학습 시 모든 데이터는 동일한 가중치를 부여 받는다.&lt;/p&gt;
&lt;p&gt;AoS 를 구하는 수식은 다음과 같이 정리할 수 있으며, 마치 sigmoid 함수를 90도 돌려놓은 듯한 모양을 가지고있다. 따라서 분류 정확도가 낮아지거나, 높아질수록 AoS 값은 극단적으로 변하게된다.&lt;/p&gt;
&lt;p&gt;$$
\text{AoS} = \frac{1}{2} \text{log} (\frac{1 - \text{total error}}{\text{total error}})
$$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/adaboost_2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. Amount of Say&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;오분류된 데이터를 강조한다는 룰에 따라 데이터 별 가중치는 잘못 분류된 경우 증가하고, 제대로 분류된 경우 감소하게 된다.&lt;/p&gt;
&lt;p&gt;오분류된 데이터의 가중치를 증가시키는 경우, 새로운 가중치는 다음과 같이 정의된다.&lt;/p&gt;
&lt;p&gt;$$
\text{새로운 샘플 가중치} = \text{현재 샘플 가중치} \cdot e^{AoS}
$$&lt;/p&gt;
&lt;p&gt;반대로 정분류된 데이터의 가중치를 감소시키는 경우, 새로운 가중치는 다음과 같이 정의된다.&lt;/p&gt;
&lt;p&gt;$$
\text{새로운 샘플 가중치} = \text{현재 샘플 가중치} \cdot e^{-AoS}
$$&lt;/p&gt;
&lt;p&gt;수식의 의미를 해석하자면, &lt;strong&gt;AoS 가 높은 경우엔 (즉 Stump 의 분류 성능이 좋은 경우) 정분류된 데이터는 해결된 문제라고 판단해 더욱 작은 가중치를 부여하고, 특정된 오분류 데이터에 더욱 높은 가중치를 부여해 이후 해결하는 과정&lt;/strong&gt;이라고 볼 수 있다. 또한 이렇게 업데이트 된 데이터 가중치는 정규화를 통해 그 합이 1 이 되도록 유지한다.&lt;/p&gt;
&lt;h3 id=&#34;의사-결정&#34;&gt;의사 결정&lt;/h3&gt;
&lt;p&gt;Amount of Say 와 Stump 의 개념을 이해했다면 AdaBoost 모델이 의사 결정을 내리는 과정을 쉽게 이해할 수 있다. 일정 개수의 Stump 가 생성된 후, &lt;strong&gt;AdaBoost 는 test 케이스에 대해 A 로 분류한 Stump 의 AoS 합과 B 로 분류한 Stump 의 AoS 합을 비교하여 더욱 큰 AoS 를 가진 클래스로 test 케이스를 분류한다&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;랜덤 포레스트와 Bagging 알고리즘의 관계와 같이 AdaBoost 는 Boosting 알고리즘의 한 케이스에 불과하며, 순차적인 학습의 개념을 이해하기 위해 해당 글이 제시한 예시로 보면 좋을 것 같다. 나아가 이후에 LightGBM, XGBoost 와 같은 Gradient Boosting 알고리즘 또한 정리해 볼 예정이다.&lt;/p&gt;
&lt;h2 id=&#34;레퍼런스&#34;&gt;레퍼런스&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=eLt4a8-316E&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=eLt4a8-316E&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=LsK-xG1cLYA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/watch?v=LsK-xG1cLYA&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
