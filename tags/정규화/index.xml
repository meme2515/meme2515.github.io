<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>정규화 on Soon&#39;s Blog</title>
        <link>https://meme2515.github.io/tags/%EC%A0%95%EA%B7%9C%ED%99%94/</link>
        <description>Recent content in 정규화 on Soon&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 30 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://meme2515.github.io/tags/%EC%A0%95%EA%B7%9C%ED%99%94/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>엘라스틱넷이란? (Elastic Net)</title>
        <link>https://meme2515.github.io/machine_learning/elasticnet/</link>
        <pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/machine_learning/elasticnet/</guid>
        <description>&lt;img src="https://meme2515.github.io/machine_learning/images/elasticnet_1.png" alt="Featured image of post 엘라스틱넷이란? (Elastic Net)" /&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;모델의 과대적합 (Overfitting) 을 방지하는 대표적인 기법으로 모델 규제 방법이 있다.&lt;/li&gt;
&lt;li&gt;회귀 모델의 경우 보통 &lt;strong&gt;모델 가중치를 제한하는 방법을 통해 규제를 가하는데&lt;/strong&gt;, 이를 &lt;strong&gt;정규화 (regularization)&lt;/strong&gt; 라 칭한다. 모델 가중치에 제한을 두는 방법론은 비단 회귀 모델 뿐 아니라 딥러닝 영역에도 적용된다 - &lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;참고 글&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;본 글에서는 대표적인 회귀 모델의 가중치 적용법인 Ridge, Lasso Regression 과 이 두 방법론을 복합적으로 활용하는 엘락스틱넷을 소개한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;regression-analysis&#34;&gt;Regression Analysis&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/elasticnet_4.jpeg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 1. &lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/linear-regression-explained-1b36f97b7572&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jason Wong - Linear Regression&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;통계적 예측 분석의 기본이 되는 회귀 분석이란, &lt;strong&gt;독립변수 $x$ 에 대응하는 종속변수 $y$ 와 가장 비슷한 값 $\hat{y}$ 을 출력하는 함수 $f(x)$ 를 찾는 과정&lt;/strong&gt;을 의미한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-regression-선형회귀식&#34;&gt;Linear Regression (선형회귀식)&lt;/h3&gt;
&lt;p&gt;$$
\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + &amp;hellip; + \theta_D x_D
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;만약 $f(x)$ 가 위와 같은 선형함수인 경우 해당 함수를 &lt;strong&gt;선형회귀모형 (Linear Regression Model)&lt;/strong&gt; 이라 칭한다.&lt;/li&gt;
&lt;li&gt;위 식에서 독립변수 $x = (x_1, x_2, &amp;hellip; , x_D)$ 는 D차원의 벡터이며, 가중치 벡터 $w = (w_0, &amp;hellip; , w_D)$ 는 함수 $f(x)$ 의 &lt;strong&gt;계수 (coefficient)&lt;/strong&gt; 이자 회귀 모형의 &lt;strong&gt;모수 (parameter)&lt;/strong&gt; 라고 부르게된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;polynomial-regression-다항회귀식&#34;&gt;Polynomial Regression (다항회귀식)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/elasticnet_6.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 2. &lt;a class=&#34;link&#34; href=&#34;https://www.javatpoint.com/machine-learning-polynomial-regression&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Java Point - ML Polynomial Regression&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;독립변수와 종속변수가 비선형 연관성을 가질 경우, 독립변수의 차수를 높여 다항식 함수를 활용할 수 있으며 이를 &lt;strong&gt;다항회귀식&lt;/strong&gt;이라 부른다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2 + \theta_4 x_2^2 + \theta_5 x_1x_2
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;한 개 이상의 독립변수가 존재할 경우 변수 간 영향을 파악하는 것이 가능하다 &lt;em&gt;(예. $x_1x_2$)&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;차수가 지나치게 높을 경우 과적합이 발생할 위험이 상승한다. 이러한 경우 Cross Validation 을 통해 적합한 차수 선정이 가능.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-normal-equation-정규방정식&#34;&gt;The Normal Equation (정규방정식)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;그렇다면 최적의 선형회귀식은 어떻게 계산할 수 있을까?&lt;/li&gt;
&lt;li&gt;데이터가 주어졌을때 최적의 추세선을 그리기위한 대표적인 방법 중 하나는 &lt;strong&gt;최소제곱법 (Ordinary Least Square, OLS)&lt;/strong&gt; 이다.&lt;/li&gt;
&lt;li&gt;최소제곱법은 &lt;strong&gt;실제 데이터 $y$ 와 추세선의 예측값 $\hat{y}$ 간 차이 (잔차, Residual) 의 제곱합을 최소화하는 추세선을 찾는 방법&lt;/strong&gt;이다. 아래 그림 참고.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/elasticnet_5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 3. &lt;a class=&#34;link&#34; href=&#34;https://gregorygundersen.com/blog/2020/01/04/ols/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Gregory Gunderson - Ordinary Least Squares&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;잔차의 제곱합을 최소화하는 선형함수의 계수는 다음과 같은 행렬연산을 통해 계산이 가능하며, 해당식을 &lt;strong&gt;정규방정식&lt;/strong&gt;이라 부른다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\theta} = (X^T X)^{-1} X^T y
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;주의할 점은 OLS 는 &lt;strong&gt;선형회귀모델의 추정 방법 중 하나일 뿐, 선형회귀모델 자체는 아니라는 점&lt;/strong&gt;이다. 선형회귀문제 해결을 위해 경사하강법 활용 또한 가능하다.&lt;/li&gt;
&lt;li&gt;하지만 OLS 방법론은 경사하강법 과는 달리 한번의 연산으로 최적 계수 산정이 가능하다.&lt;/li&gt;
&lt;li&gt;무려 1800년대 초 Gaussian Distribution 의 그 &lt;a class=&#34;link&#34; href=&#34;https://namu.wiki/w/%EC%B9%B4%EB%A5%BC%20%ED%94%84%EB%A6%AC%EB%93%9C%EB%A6%AC%ED%9E%88%20%EA%B0%80%EC%9A%B0%EC%8A%A4&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;가우스&lt;/a&gt;가 해당 기법을 활용해 소행성 세레스의 궤도를 예측하며 알려지게 되었다 (소행성의 관측위치 - 데이터 - 와 최소한의 오차를 가지는 궤도 - 선형함수 - 을 찾아라&amp;hellip;!) - &lt;a class=&#34;link&#34; href=&#34;https://ko.wikipedia.org/wiki/%EC%84%B8%EB%A0%88%EC%8A%A4_%28%EC%99%9C%ED%96%89%EC%84%B1%29&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;출처&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ridge-l2-regression&#34;&gt;Ridge (L2) Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ridge Regression 은 통계학에서 능형회귀, 또는 Tikhonov Regularization 이라고 불리며, 다음과 같은 비용함수를 활용해 회귀식을 추정한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
J(\theta) = MSE(\theta) + \alpha \frac{1}{2} \sum_{i=1}^{n} \theta_i^2
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;뒷단의 정규화식 $\alpha \frac{1}{2} \sum_{i=1}^{n} \theta_i^2$ 은 모델 학습과정에서만 감안될 뿐, 실제 예측 과정에서 제외되어야 한다.&lt;/li&gt;
&lt;li&gt;Ridge Regression 이 적용된 선형 함수는 데이터에 대한 잔차뿐 아니라 계수값 또한 최소화하게 되며, 이는 &lt;strong&gt;종속변수 예측에 비핵심적인 계수값을 줄이는 효과&lt;/strong&gt;를 가진다.&lt;/li&gt;
&lt;li&gt;하이퍼파라미터 $\alpha$ 를 통해 규제량을 통제할 수 있으며, 이를 매우 큰 값으로 설정할 경우 모든 계수값에 0 에 근접해지는 효과를 가진다.&lt;/li&gt;
&lt;li&gt;Ridge Regression 수행을 위한 정규방정식은 다음과 같다 ($A$ 는 (n+1) x (n+1) 크기의 Identity Matrix 이며, 가장 왼쪽 상단의 값이 $0$ 이다).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\hat{\theta} = (X^T X + \alpha A)^{-1} X^T y
$$&lt;/p&gt;
&lt;h2 id=&#34;lasso-l1-regression&#34;&gt;Lasso (L1) Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lasso (Least Absolute Shrinkage and Selection Operator) Regression 은 기본적으로 Ridge Regression 과 동일하지만 계수의 제곱값이 아닌 절대값을 활용한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
J(\theta) = MSE(\theta) + \alpha \sum_{i=1}^{n} |\theta_i|
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lasso Regression 의 가장 중요한 성질은 &lt;strong&gt;종속변수 예측에 비핵심적인 변수를 모델에서 제외하는 효과&lt;/strong&gt;를 가진다는 점이다. 이는 해당 변수의 계수를 낮게 산정하지만, 제외하지는 않는 Ridge Regression 과 차이를 가진다.&lt;/li&gt;
&lt;li&gt;상기된 효과는 일종의 Feature Selection 과정으로 해석하는 것이 가능하다.&lt;/li&gt;
&lt;li&gt;이러한 효과가 나타나는 이유는 &lt;strong&gt;L1 Norm (절대값) 은 크기에 관계없이 모든 계수를 동일하게 취급하는 반면, L2 Norm (제곱값) 은 크기가 큰 계수에 더욱 큰 가중치를 부여하기 때문&lt;/strong&gt;이다. 따라서 최적화 시 계수의 감소 속도에 차이를 가지게 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;https://meme2515.github.io/machine_learning/images/elasticnet_7.pbm&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fig 4. &lt;a class=&#34;link&#34; href=&#34;https://www.researchgate.net/figure/Parameter-norm-penalties-L2-norm-regularization-left-and-L1-norm-regularization_fig2_355020694&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Research Gate - L1 vs. L2 Norm&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;위 그림은 L1 vs. L2 정규화 함수가 동일한 손실값을 산정하는 영역을 나타낸다. 제곱값은 원형으로, 절대값은 사각형으로 묘사되는 것을 확인할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;elastic-net&#34;&gt;Elastic Net&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;엘라스틱넷&lt;/strong&gt;은 Ridge Regression 과 Lasso Regression 을 복합적으로 활용하는 정규화 기법이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
J(\theta) = MSE(\theta) + r\alpha \sum_{i=1}^{n} |\theta_i| + \frac{1-r}{2}\alpha \sum_{i=1}^{n} \theta_i^2
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;하이퍼파라미터 $r$ 을 활용하여 각 기법의 영향도를 조절할 수 있으며, 한가지 기법만을 활용하는 것 또한 가능하다.&lt;/li&gt;
&lt;li&gt;일반적인 회귀 문제에선 정규화를 반드시 포함하는 것이 권장된다. Ridge Regression 은 좋은 시작 지점이나, feature 선별이 필요한 경우 Lasso 또는 엘라스틱넷을 활용하는 편이 좋다.&lt;/li&gt;
&lt;li&gt;Lasso Regression 은 feature 수가 지나치게 많거나 multicollinearity 가 존재하는 경우 이상행동을 보일 수 있으며, 엘라스틱넷의 경우 이러한 문제가 덜한 편.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;O&amp;rsquo;Reilly - Hands-On Machine Learning with Scikit-Learn, Keras &amp;amp; Tensorflow&lt;/li&gt;
&lt;li&gt;StatQuest - Regularization with Regression &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=Q81RR3yKn30&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[1]&lt;/a&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=NGf0voTMlcs&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[2]&lt;/a&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=1dKRdX9bfIo&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[3]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://yganalyst.github.io/ml/ML_chap3-4/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Handson ML - 릿지, 라쏘, 엘라스틱 넷&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://datascienceschool.net/03%20machine%20learning/04.02%20%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D%EC%9D%98%20%EA%B8%B0%EC%B4%88.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;데이터 사이언스 스쿨 - 선형회귀분석의 기초&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>배치정규화 (Batch Normalization) 란?</title>
        <link>https://meme2515.github.io/neural_network/batchnorm/</link>
        <pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate>
        
        <guid>https://meme2515.github.io/neural_network/batchnorm/</guid>
        <description>&lt;img src="https://meme2515.github.io/neural_network/images/batchnorm.png" alt="Featured image of post 배치정규화 (Batch Normalization) 란?" /&gt;&lt;h2 id=&#34;관련-논문위키-링크&#34;&gt;관련 논문/위키 링크&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1502.03167&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1805.11604&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;How Does Batch Normalization Help Optimization?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Batch_normalization&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Wikipedia - Batch Normalization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;tldr&#34;&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;딥러닝 모델의 mini-batch 학습은, 학습 단계 별 데이터의 분포가 서로 다르다는 점에서 그 복잡성이 올라가고는 한다. 배치정규화 논문이 공개되기 전에는 이 문제를 해결하기 위해 단순히 input data $X$ 를 정규화 하는것에 그쳤으나, 해당 논문을 개재한 Google 팀은 각 중간 레이어의 아웃풋에 또한 정규화를 적용함으로 모델의 학습속도를 끌어올릴 수 있다는 점을 발견했다. 이와 같은 레이어 별 정규화 과정은 배치 스텝마다 별도로 적용되어야 하며 &lt;em&gt;(input data $X$에 대한 정규화는 전체 분포에 대한 정보가 있기때문에 일괄적으로 이루어질 수 있지만 중간 레이어의 결과값은 그렇지 못함)&lt;/em&gt;, 따라서 이를 &lt;strong&gt;배치정규화&lt;/strong&gt;라 칭한다.&lt;/p&gt;
&lt;h2 id=&#34;internal-covariate-shift&#34;&gt;Internal Covariate Shift&lt;/h2&gt;
&lt;p&gt;Google 팀이 중간레이어의 아웃풋을 정규화하게 된 배경에는 그들이 internal covariate shift 라고 명명한 문제가 존재한다. 비신경망 머신러닝 모델에 서로 다른 분포를 가진 데이터를 넣을떄 발생하는 &lt;a class=&#34;link&#34; href=&#34;https://www.google.com/search?client=firefox-b-d&amp;amp;q=covariate&amp;#43;shift&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;covariate shift&lt;/a&gt; 라는 문제를 중간레이어 개념에 도입한 것인데, 특히 hyperbolic tangent, sigmoid와 같은 &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Activation_function&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;비선형 활성화 함수 (non-linear activation function)&lt;/a&gt; 를 무겁게 사용하는 딥러닝 모델의 경우 이와 같은 분포 차이에 취약해질 여지가 많다.&lt;/p&gt;
&lt;p&gt;$$
g(x) = \frac{1}{1 + e^{-x}}
$$&lt;/p&gt;
&lt;p&gt;위의 sigmoid 함수에서 input 값 $x$가 조금만 올라가거나 내려가도 학습 속도는 기하급수적으로 느려지게된다는 점을 기억할 것이다 &lt;em&gt;(함수의 결과값이 0에 가까워지며 그 기울기가 작아지기 때문)&lt;/em&gt;. 분포차가 심한 데이터를 단계별로 학습할때 이와 같은 문제로 인해 convergence를 찾는 과정이 심각하게 느려질 수 있다.&lt;/p&gt;
&lt;p&gt;또 하나의 문제는 레이어를 통과할수록 배치 간 데이터의 분포가 점차 더 큰 차이를 가지게 된다는 점이다. 신경망 구조가 워낙 복잡하기도 하지만, 이전 레이어의 가중치 (weight) 와 편향 (bias) 이 학습과정에서 계속 업데이트 되기 때문인데, 이쯤되면 중간레이어의 정규화 없이 학습이 이루어진다는게 오히려 이상하게 보인다.&lt;/p&gt;
&lt;h2 id=&#34;방법론&#34;&gt;방법론&lt;/h2&gt;
&lt;p&gt;일반적인 배치정규화는 activation function 의 아웃풋 $a$ 가 아닌 인풋 $z$ 에 적용된다. 비교적 일정하고, 적정한 범위내의 데이터를 activation function 에 집어넣어 activation layer 내 가능한 많은 노드가 제 역할을 하게하자는 취지이다. 한 개 레이어에서 배치정규화를 수행하는 방법은 다음과 같다.&lt;/p&gt;
&lt;p&gt;$$
\mu = \frac{1}{m} \sum_{i} z^i
$$&lt;/p&gt;
&lt;p&gt;$$
\sigma^2 = \frac{1}{m} \sum_{i} (z_i - \mu)^2
$$&lt;/p&gt;
&lt;p&gt;우선 선형함수 $z$ 의 평균 $\mu$ 와 분산 $\sigma^2$ 를 구한다. 해당 값들은 레이어, 배치 별로 그 값이 다르기 때문에 학습, 예측 단계에서 매번 계산이 필요하다.&lt;/p&gt;
&lt;p&gt;$$
z_{norm}^i = \frac{z^i - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$&lt;/p&gt;
&lt;p&gt;정규화 수식을 이용하여 $z_{norm}$ 값을 구한다. 여기서 수식의 분모에 있는 $\epsilon$ 은 $\sigma^2$ 가 $0$ 일 경우에 대비한 아주 작은 safety term 이다. 이로 인해 $z_{norm}$ 은 평균이 $0$ 이며, 표준편차가 $1$ 에 해당하는 분포를 가지게된다.&lt;/p&gt;
&lt;p&gt;Input에 대한 정규화 처리는 이 단계에서 끝나겠지만 배치정규화는 다음과 같이 평균값과 표준편차에 대한 자유도를 주게된다. 처음 접했을때 다소 헷갈렸던 부분인데 데이터에 관계 없이 고른 분포를 추출하는 과정이라고 생각하면된다.&lt;/p&gt;
&lt;p&gt;$$
\tilde{z^i} = \Gamma z_{norm}^i + \Beta
$$&lt;/p&gt;
&lt;p&gt;여기서 $\Gamma$ 와 $\Beta$ 는 학습을 통해 최적값에 수렴하게된다. 이후 레이어, 배치 별 정규화가 적용된 $\tilde{z^i}$ 를 활성함수의 인풋으로 사용하면 배치정규화가 적용된 것이다.&lt;/p&gt;
&lt;h2 id=&#34;기타-효과&#34;&gt;기타 효과&lt;/h2&gt;
&lt;p&gt;구체적으로 배치정규화가 모델의 학습과정을 개선시키는 방법은 다음과 같이 정리할 수 있다.&lt;/p&gt;
&lt;h3 id=&#34;1-학습-속도-개선&#34;&gt;1. 학습 속도 개선&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://meme2515.github.io/neural_network/images/batchnorm_2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alt text&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;정규화된 분포는 어떻게 학습 속도를 개선할까? 위 그림에서 왼쪽 그래프는 정규화가 적용되지 않은 경우의 손실함수를, 오른쪽 그래프는 정규화가 적용된 경우의 손실함수를 시각화 하고있다. 왼쪽의 경우 전반적인 손실함수 결과값이 $x$ 축 변수보다 $y$ 축 변수의 움직임에 더 민감한 부분을 확인할 수 있는데, 따라서 큰 학습속도 $\alpha$ 를 적용할 경우 방향성을 잃어 최적값을 찾지 못하는 문제가 발생할 여지를 가지게 된다. &lt;strong&gt;두 변수의 스케일 차이가 학습률에 제약을 가져오는 것이다&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;반면 오른쪽 그래프에서는 손실함수의 결과값이 두 개 변수에 유사한 민감도를 가지고 있는 점을 확인할 수 있다. 물론 이 경우 또한 지나치게 큰 $\alpha$ 값은 문제를 야기하겠지만, 전자의 경우에 비해 그 정도가 개선되었다는 점을 독자는 시각적으로 확인이 가능할 것이다.&lt;/p&gt;
&lt;h3 id=&#34;2-초기값에-대한-내성&#34;&gt;2. 초기값에 대한 내성&lt;/h3&gt;
&lt;p&gt;위 그림을 다시 참조하자. 왼쪽 그래프의 경우 초기값이 타원의 오른쪽 끝에 있을 경우와, 중하단에 있을 경우 중심점 (최적값) 으로 부터의 상당한 거리차가 발생한다. 이는 모델의 초기값에 따른 학습속도 차이가 발생할 수 있음을 의미한다. 오른쪽 그래프 또한 이러한 문제점을 어느정도 안고 있지만, 적어도 동일한 붉은 원 안에서는 거리차가 발생하지 않는다는 점을 확인할 수 있다.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
